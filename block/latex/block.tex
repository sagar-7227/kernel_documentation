%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[a4paper,11pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}


\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}
\sphinxsetup{
        hmargin=0.5in, vmargin=1in,
        parsedliteralwraps=true,
        verbatimhintsturnover=false,
    }
\fvset{fontsize=\small}
\usepackage{geometry}
\usepackage{setspace}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}


        % Use some font with UTF-8 support with XeLaTeX
        \usepackage{fontspec}
        \setsansfont{DejaVu Sans}
        \setromanfont{DejaVu Serif}
        \setmonofont{DejaVu Sans Mono}
    
        % Load kerneldoc specific LaTeX settings
	\input{kerneldoc-preamble.sty}


\title{Linux Block Documentation}
\date{Aug 21, 2023}
\release{5.4.10-custom}
\author{The kernel development community}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{BFQ (Budget Fair Queueing)}
\label{\detokenize{bfq-iosched:bfq-budget-fair-queueing}}\label{\detokenize{bfq-iosched::doc}}
BFQ is a proportional\sphinxhyphen{}share I/O scheduler, with some extra
low\sphinxhyphen{}latency capabilities. In addition to cgroups support (blkio or io
controllers), BFQ\textquotesingle{}s main features are:
\begin{itemize}
\item {} 
BFQ guarantees a high system and application responsiveness, and a
low latency for time\sphinxhyphen{}sensitive applications, such as audio or video
players;

\item {} 
BFQ distributes bandwidth, and not just time, among processes or
groups (switching back to time distribution when needed to keep
throughput high).

\end{itemize}

In its default configuration, BFQ privileges latency over
throughput. So, when needed for achieving a lower latency, BFQ builds
schedules that may lead to a lower throughput. If your main or only
goal, for a given device, is to achieve the maximum\sphinxhyphen{}possible
throughput at all times, then do switch off all low\sphinxhyphen{}latency heuristics
for that device, by setting low\_latency to 0. See Section 3 for
details on how to configure BFQ for the desired tradeoff between
latency and throughput, or on how to maximize throughput.

As every I/O scheduler, BFQ adds some overhead to per\sphinxhyphen{}I/O\sphinxhyphen{}request
processing. To give an idea of this overhead, the total,
single\sphinxhyphen{}lock\sphinxhyphen{}protected, per\sphinxhyphen{}request processing time of BFQ\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}i.e., the
sum of the execution times of the request insertion, dispatch and
completion hooks\sphinxhyphen{}\sphinxhyphen{}\sphinxhyphen{}is, e.g., 1.9 us on an Intel Core \sphinxhref{mailto:i7-2760QM@2.40GHz}{i7\sphinxhyphen{}2760QM@2.40GHz}
(dated CPU for notebooks; time measured with simple code
instrumentation, and using the throughput\sphinxhyphen{}sync.sh script of the S
suite {[}1{]}, in performance\sphinxhyphen{}profiling mode). To put this result into
context, the total, single\sphinxhyphen{}lock\sphinxhyphen{}protected, per\sphinxhyphen{}request execution time
of the lightest I/O scheduler available in blk\sphinxhyphen{}mq, mq\sphinxhyphen{}deadline, is 0.7
us (mq\sphinxhyphen{}deadline is \textasciitilde{}800 LOC, against \textasciitilde{}10500 LOC for BFQ).

Scheduling overhead further limits the maximum IOPS that a CPU can
process (already limited by the execution of the rest of the I/O
stack). To give an idea of the limits with BFQ, on slow or average
CPUs, here are, first, the limits of BFQ for three different CPUs, on,
respectively, an average laptop, an old desktop, and a cheap embedded
system, in case full hierarchical support is enabled (i.e.,
CONFIG\_BFQ\_GROUP\_IOSCHED is set), but CONFIG\_BFQ\_CGROUP\_DEBUG is not
set (Section 4\sphinxhyphen{}2):
\sphinxhyphen{} Intel i7\sphinxhyphen{}4850HQ: 400 KIOPS
\sphinxhyphen{} AMD A8\sphinxhyphen{}3850: 250 KIOPS
\sphinxhyphen{} ARM CortexTM\sphinxhyphen{}A53 Octa\sphinxhyphen{}core: 80 KIOPS

If CONFIG\_BFQ\_CGROUP\_DEBUG is set (and of course full hierarchical
support is enabled), then the sustainable throughput with BFQ
decreases, because all blkio.bfq* statistics are created and updated
(Section 4\sphinxhyphen{}2). For BFQ, this leads to the following maximum
sustainable throughputs, on the same systems as above:
\sphinxhyphen{} Intel i7\sphinxhyphen{}4850HQ: 310 KIOPS
\sphinxhyphen{} AMD A8\sphinxhyphen{}3850: 200 KIOPS
\sphinxhyphen{} ARM CortexTM\sphinxhyphen{}A53 Octa\sphinxhyphen{}core: 56 KIOPS

BFQ works for multi\sphinxhyphen{}queue devices too.


\section{1. When may BFQ be useful?}
\label{\detokenize{bfq-iosched:when-may-bfq-be-useful}}
BFQ provides the following benefits on personal and server systems.


\subsection{1\sphinxhyphen{}1 Personal systems}
\label{\detokenize{bfq-iosched:personal-systems}}

\subsubsection{Low latency for interactive applications}
\label{\detokenize{bfq-iosched:low-latency-for-interactive-applications}}
Regardless of the actual background workload, BFQ guarantees that, for
interactive tasks, the storage device is virtually as responsive as if
it was idle. For example, even if one or more of the following
background workloads are being executed:
\begin{itemize}
\item {} 
one or more large files are being read, written or copied,

\item {} 
a tree of source files is being compiled,

\item {} 
one or more virtual machines are performing I/O,

\item {} 
a software update is in progress,

\item {} 
indexing daemons are scanning filesystems and updating their
databases,

\end{itemize}

starting an application or loading a file from within an application
takes about the same time as if the storage device was idle. As a
comparison, with CFQ, NOOP or DEADLINE, and in the same conditions,
applications experience high latencies, or even become unresponsive
until the background workload terminates (also on SSDs).


\subsubsection{Low latency for soft real\sphinxhyphen{}time applications}
\label{\detokenize{bfq-iosched:low-latency-for-soft-real-time-applications}}
Also soft real\sphinxhyphen{}time applications, such as audio and video
players/streamers, enjoy a low latency and a low drop rate, regardless
of the background I/O workload. As a consequence, these applications
do not suffer from almost any glitch due to the background workload.


\subsubsection{Higher speed for code\sphinxhyphen{}development tasks}
\label{\detokenize{bfq-iosched:higher-speed-for-code-development-tasks}}
If some additional workload happens to be executed in parallel, then
BFQ executes the I/O\sphinxhyphen{}related components of typical code\sphinxhyphen{}development
tasks (compilation, checkout, merge, ...) much more quickly than CFQ,
NOOP or DEADLINE.


\subsubsection{High throughput}
\label{\detokenize{bfq-iosched:high-throughput}}
On hard disks, BFQ achieves up to 30\% higher throughput than CFQ, and
up to 150\% higher throughput than DEADLINE and NOOP, with all the
sequential workloads considered in our tests. With random workloads,
and with all the workloads on flash\sphinxhyphen{}based devices, BFQ achieves,
instead, about the same throughput as the other schedulers.


\subsubsection{Strong fairness, bandwidth and delay guarantees}
\label{\detokenize{bfq-iosched:strong-fairness-bandwidth-and-delay-guarantees}}
BFQ distributes the device throughput, and not just the device time,
among I/O\sphinxhyphen{}bound applications in proportion their weights, with any
workload and regardless of the device parameters. From these bandwidth
guarantees, it is possible to compute tight per\sphinxhyphen{}I/O\sphinxhyphen{}request delay
guarantees by a simple formula. If not configured for strict service
guarantees, BFQ switches to time\sphinxhyphen{}based resource sharing (only) for
applications that would otherwise cause a throughput loss.


\subsection{1\sphinxhyphen{}2 Server systems}
\label{\detokenize{bfq-iosched:server-systems}}
Most benefits for server systems follow from the same service
properties as above. In particular, regardless of whether additional,
possibly heavy workloads are being served, BFQ guarantees:
\begin{itemize}
\item {} 
audio and video\sphinxhyphen{}streaming with zero or very low jitter and drop
rate;

\item {} 
fast retrieval of WEB pages and embedded objects;

\item {} 
real\sphinxhyphen{}time recording of data in live\sphinxhyphen{}dumping applications (e.g.,
packet logging);

\item {} 
responsiveness in local and remote access to a server.

\end{itemize}


\section{2. How does BFQ work?}
\label{\detokenize{bfq-iosched:how-does-bfq-work}}
BFQ is a proportional\sphinxhyphen{}share I/O scheduler, whose general structure,
plus a lot of code, are borrowed from CFQ.
\begin{itemize}
\item {} 
Each process doing I/O on a device is associated with a weight and a
\sphinxtitleref{(bfq\_)queue}.

\item {} 
BFQ grants exclusive access to the device, for a while, to one queue
(process) at a time, and implements this service model by
associating every queue with a budget, measured in number of
sectors.
\begin{itemize}
\item {} 
After a queue is granted access to the device, the budget of the
queue is decremented, on each request dispatch, by the size of the
request.

\item {} 
The in\sphinxhyphen{}service queue is expired, i.e., its service is suspended,
only if one of the following events occurs: 1) the queue finishes
its budget, 2) the queue empties, 3) a "budget timeout" fires.
\begin{itemize}
\item {} 
The budget timeout prevents processes doing random I/O from
holding the device for too long and dramatically reducing
throughput.

\item {} 
Actually, as in CFQ, a queue associated with a process issuing
sync requests may not be expired immediately when it empties. In
contrast, BFQ may idle the device for a short time interval,
giving the process the chance to go on being served if it issues
a new request in time. Device idling typically boosts the
throughput on rotational devices and on non\sphinxhyphen{}queueing flash\sphinxhyphen{}based
devices, if processes do synchronous and sequential I/O. In
addition, under BFQ, device idling is also instrumental in
guaranteeing the desired throughput fraction to processes
issuing sync requests (see the description of the slice\_idle
tunable in this document, or {[}1, 2{]}, for more details).
\begin{itemize}
\item {} 
With respect to idling for service guarantees, if several
processes are competing for the device at the same time, but
all processes and groups have the same weight, then BFQ
guarantees the expected throughput distribution without ever
idling the device. Throughput is thus as high as possible in
this common scenario.

\end{itemize}

\end{itemize}
\begin{itemize}
\item {} 
On flash\sphinxhyphen{}based storage with internal queueing of commands
(typically NCQ), device idling happens to be always detrimental
for throughput. So, with these devices, BFQ performs idling
only when strictly needed for service guarantees, i.e., for
guaranteeing low latency or fairness. In these cases, overall
throughput may be sub\sphinxhyphen{}optimal. No solution currently exists to
provide both strong service guarantees and optimal throughput
on devices with internal queueing.

\end{itemize}

\item {} 
If low\sphinxhyphen{}latency mode is enabled (default configuration), BFQ
executes some special heuristics to detect interactive and soft
real\sphinxhyphen{}time applications (e.g., video or audio players/streamers),
and to reduce their latency. The most important action taken to
achieve this goal is to give to the queues associated with these
applications more than their fair share of the device
throughput. For brevity, we call just "weight\sphinxhyphen{}raising" the whole
sets of actions taken by BFQ to privilege these queues. In
particular, BFQ provides a milder form of weight\sphinxhyphen{}raising for
interactive applications, and a stronger form for soft real\sphinxhyphen{}time
applications.

\item {} 
BFQ automatically deactivates idling for queues born in a burst of
queue creations. In fact, these queues are usually associated with
the processes of applications and services that benefit mostly
from a high throughput. Examples are systemd during boot, or git
grep.

\item {} 
As CFQ, BFQ merges queues performing interleaved I/O, i.e.,
performing random I/O that becomes mostly sequential if
merged. Differently from CFQ, BFQ achieves this goal with a more
reactive mechanism, called Early Queue Merge (EQM). EQM is so
responsive in detecting interleaved I/O (cooperating processes),
that it enables BFQ to achieve a high throughput, by queue
merging, even for queues for which CFQ needs a different
mechanism, preemption, to get a high throughput. As such EQM is a
unified mechanism to achieve a high throughput with interleaved
I/O.

\item {} 
Queues are scheduled according to a variant of WF2Q+, named
B\sphinxhyphen{}WF2Q+, and implemented using an augmented rb\sphinxhyphen{}tree to preserve an
O(log N) overall complexity.  See {[}2{]} for more details. B\sphinxhyphen{}WF2Q+ is
also ready for hierarchical scheduling, details in Section 4.

\item {} 
B\sphinxhyphen{}WF2Q+ guarantees a tight deviation with respect to an ideal,
perfectly fair, and smooth service. In particular, B\sphinxhyphen{}WF2Q+
guarantees that each queue receives a fraction of the device
throughput proportional to its weight, even if the throughput
fluctuates, and regardless of: the device parameters, the current
workload and the budgets assigned to the queue.

\item {} 
The last, budget\sphinxhyphen{}independence, property (although probably
counterintuitive in the first place) is definitely beneficial, for
the following reasons:
\begin{itemize}
\item {} 
First, with any proportional\sphinxhyphen{}share scheduler, the maximum
deviation with respect to an ideal service is proportional to
the maximum budget (slice) assigned to queues. As a consequence,
BFQ can keep this deviation tight not only because of the
accurate service of B\sphinxhyphen{}WF2Q+, but also because BFQ \sphinxstyleemphasis{does not}
need to assign a larger budget to a queue to let the queue
receive a higher fraction of the device throughput.

\item {} 
Second, BFQ is free to choose, for every process (queue), the
budget that best fits the needs of the process, or best
leverages the I/O pattern of the process. In particular, BFQ
updates queue budgets with a simple feedback\sphinxhyphen{}loop algorithm that
allows a high throughput to be achieved, while still providing
tight latency guarantees to time\sphinxhyphen{}sensitive applications. When
the in\sphinxhyphen{}service queue expires, this algorithm computes the next
budget of the queue so as to:
\begin{itemize}
\item {} 
Let large budgets be eventually assigned to the queues
associated with I/O\sphinxhyphen{}bound applications performing sequential
I/O: in fact, the longer these applications are served once
got access to the device, the higher the throughput is.

\item {} 
Let small budgets be eventually assigned to the queues
associated with time\sphinxhyphen{}sensitive applications (which typically
perform sporadic and short I/O), because, the smaller the
budget assigned to a queue waiting for service is, the sooner
B\sphinxhyphen{}WF2Q+ will serve that queue (Subsec 3.3 in {[}2{]}).

\end{itemize}

\end{itemize}

\end{itemize}

\item {} 
If several processes are competing for the device at the same time,
but all processes and groups have the same weight, then BFQ
guarantees the expected throughput distribution without ever idling
the device. It uses preemption instead. Throughput is then much
higher in this common scenario.

\item {} 
ioprio classes are served in strict priority order, i.e.,
lower\sphinxhyphen{}priority queues are not served as long as there are
higher\sphinxhyphen{}priority queues.  Among queues in the same class, the
bandwidth is distributed in proportion to the weight of each
queue. A very thin extra bandwidth is however guaranteed to
the Idle class, to prevent it from starving.

\end{itemize}


\section{3. What are BFQ\textquotesingle{}s tunables and how to properly configure BFQ?}
\label{\detokenize{bfq-iosched:what-are-bfq-s-tunables-and-how-to-properly-configure-bfq}}
Most BFQ tunables affect service guarantees (basically latency and
fairness) and throughput. For full details on how to choose the
desired tradeoff between service guarantees and throughput, see the
parameters slice\_idle, strict\_guarantees and low\_latency. For details
on how to maximise throughput, see slice\_idle, timeout\_sync and
max\_budget. The other performance\sphinxhyphen{}related parameters have been
inherited from, and have been preserved mostly for compatibility with
CFQ. So far, no performance improvement has been reported after
changing the latter parameters in BFQ.

In particular, the tunables back\_seek\sphinxhyphen{}max, back\_seek\_penalty,
fifo\_expire\_async and fifo\_expire\_sync below are the same as in
CFQ. Their description is just copied from that for CFQ. Some
considerations in the description of slice\_idle are copied from CFQ
too.


\subsection{per\sphinxhyphen{}process ioprio and weight}
\label{\detokenize{bfq-iosched:per-process-ioprio-and-weight}}
Unless the cgroups interface is used (see "4. BFQ group scheduling"),
weights can be assigned to processes only indirectly, through I/O
priorities, and according to the relation:
weight = (IOPRIO\_BE\_NR \sphinxhyphen{} ioprio) * 10.

Beware that, if low\sphinxhyphen{}latency is set, then BFQ automatically raises the
weight of the queues associated with interactive and soft real\sphinxhyphen{}time
applications. Unset this tunable if you need/want to control weights.


\subsection{slice\_idle}
\label{\detokenize{bfq-iosched:slice-idle}}
This parameter specifies how long BFQ should idle for next I/O
request, when certain sync BFQ queues become empty. By default
slice\_idle is a non\sphinxhyphen{}zero value. Idling has a double purpose: boosting
throughput and making sure that the desired throughput distribution is
respected (see the description of how BFQ works, and, if needed, the
papers referred there).

As for throughput, idling can be very helpful on highly seeky media
like single spindle SATA/SAS disks where we can cut down on overall
number of seeks and see improved throughput.

Setting slice\_idle to 0 will remove all the idling on queues and one
should see an overall improved throughput on faster storage devices
like multiple SATA/SAS disks in hardware RAID configuration, as well
as flash\sphinxhyphen{}based storage with internal command queueing (and
parallelism).

So depending on storage and workload, it might be useful to set
slice\_idle=0.  In general for SATA/SAS disks and software RAID of
SATA/SAS disks keeping slice\_idle enabled should be useful. For any
configurations where there are multiple spindles behind single LUN
(Host based hardware RAID controller or for storage arrays), or with
flash\sphinxhyphen{}based fast storage, setting slice\_idle=0 might end up in better
throughput and acceptable latencies.

Idling is however necessary to have service guarantees enforced in
case of differentiated weights or differentiated I/O\sphinxhyphen{}request lengths.
To see why, suppose that a given BFQ queue A must get several I/O
requests served for each request served for another queue B. Idling
ensures that, if A makes a new I/O request slightly after becoming
empty, then no request of B is dispatched in the middle, and thus A
does not lose the possibility to get more than one request dispatched
before the next request of B is dispatched. Note that idling
guarantees the desired differentiated treatment of queues only in
terms of I/O\sphinxhyphen{}request dispatches. To guarantee that the actual service
order then corresponds to the dispatch order, the strict\_guarantees
tunable must be set too.

There is an important flipside for idling: apart from the above cases
where it is beneficial also for throughput, idling can severely impact
throughput. One important case is random workload. Because of this
issue, BFQ tends to avoid idling as much as possible, when it is not
beneficial also for throughput (as detailed in Section 2). As a
consequence of this behavior, and of further issues described for the
strict\_guarantees tunable, short\sphinxhyphen{}term service guarantees may be
occasionally violated. And, in some cases, these guarantees may be
more important than guaranteeing maximum throughput. For example, in
video playing/streaming, a very low drop rate may be more important
than maximum throughput. In these cases, consider setting the
strict\_guarantees parameter.


\subsection{slice\_idle\_us}
\label{\detokenize{bfq-iosched:slice-idle-us}}
Controls the same tuning parameter as slice\_idle, but in microseconds.
Either tunable can be used to set idling behavior.  Afterwards, the
other tunable will reflect the newly set value in sysfs.


\subsection{strict\_guarantees}
\label{\detokenize{bfq-iosched:strict-guarantees}}
If this parameter is set (default: unset), then BFQ
\begin{itemize}
\item {} 
always performs idling when the in\sphinxhyphen{}service queue becomes empty;

\item {} 
forces the device to serve one I/O request at a time, by dispatching a
new request only if there is no outstanding request.

\end{itemize}

In the presence of differentiated weights or I/O\sphinxhyphen{}request sizes, both
the above conditions are needed to guarantee that every BFQ queue
receives its allotted share of the bandwidth. The first condition is
needed for the reasons explained in the description of the slice\_idle
tunable.  The second condition is needed because all modern storage
devices reorder internally\sphinxhyphen{}queued requests, which may trivially break
the service guarantees enforced by the I/O scheduler.

Setting strict\_guarantees may evidently affect throughput.


\subsection{back\_seek\_max}
\label{\detokenize{bfq-iosched:back-seek-max}}
This specifies, given in Kbytes, the maximum "distance" for backward seeking.
The distance is the amount of space from the current head location to the
sectors that are backward in terms of distance.

This parameter allows the scheduler to anticipate requests in the "backward"
direction and consider them as being the "next" if they are within this
distance from the current head location.


\subsection{back\_seek\_penalty}
\label{\detokenize{bfq-iosched:back-seek-penalty}}
This parameter is used to compute the cost of backward seeking. If the
backward distance of request is just 1/back\_seek\_penalty from a "front"
request, then the seeking cost of two requests is considered equivalent.

So scheduler will not bias toward one or the other request (otherwise scheduler
will bias toward front request). Default value of back\_seek\_penalty is 2.


\subsection{fifo\_expire\_async}
\label{\detokenize{bfq-iosched:fifo-expire-async}}
This parameter is used to set the timeout of asynchronous requests. Default
value of this is 250ms.


\subsection{fifo\_expire\_sync}
\label{\detokenize{bfq-iosched:fifo-expire-sync}}
This parameter is used to set the timeout of synchronous requests. Default
value of this is 125ms. In case to favor synchronous requests over asynchronous
one, this value should be decreased relative to fifo\_expire\_async.


\subsection{low\_latency}
\label{\detokenize{bfq-iosched:low-latency}}
This parameter is used to enable/disable BFQ\textquotesingle{}s low latency mode. By
default, low latency mode is enabled. If enabled, interactive and soft
real\sphinxhyphen{}time applications are privileged and experience a lower latency,
as explained in more detail in the description of how BFQ works.

DISABLE this mode if you need full control on bandwidth
distribution. In fact, if it is enabled, then BFQ automatically
increases the bandwidth share of privileged applications, as the main
means to guarantee a lower latency to them.

In addition, as already highlighted at the beginning of this document,
DISABLE this mode if your only goal is to achieve a high throughput.
In fact, privileging the I/O of some application over the rest may
entail a lower throughput. To achieve the highest\sphinxhyphen{}possible throughput
on a non\sphinxhyphen{}rotational device, setting slice\_idle to 0 may be needed too
(at the cost of giving up any strong guarantee on fairness and low
latency).


\subsection{timeout\_sync}
\label{\detokenize{bfq-iosched:timeout-sync}}
Maximum amount of device time that can be given to a task (queue) once
it has been selected for service. On devices with costly seeks,
increasing this time usually increases maximum throughput. On the
opposite end, increasing this time coarsens the granularity of the
short\sphinxhyphen{}term bandwidth and latency guarantees, especially if the
following parameter is set to zero.


\subsection{max\_budget}
\label{\detokenize{bfq-iosched:max-budget}}
Maximum amount of service, measured in sectors, that can be provided
to a BFQ queue once it is set in service (of course within the limits
of the above timeout). According to what said in the description of
the algorithm, larger values increase the throughput in proportion to
the percentage of sequential I/O requests issued. The price of larger
values is that they coarsen the granularity of short\sphinxhyphen{}term bandwidth
and latency guarantees.

The default value is 0, which enables auto\sphinxhyphen{}tuning: BFQ sets max\_budget
to the maximum number of sectors that can be served during
timeout\_sync, according to the estimated peak rate.

For specific devices, some users have occasionally reported to have
reached a higher throughput by setting max\_budget explicitly, i.e., by
setting max\_budget to a higher value than 0. In particular, they have
set max\_budget to higher values than those to which BFQ would have set
it with auto\sphinxhyphen{}tuning. An alternative way to achieve this goal is to
just increase the value of timeout\_sync, leaving max\_budget equal to 0.


\section{4. Group scheduling with BFQ}
\label{\detokenize{bfq-iosched:group-scheduling-with-bfq}}
BFQ supports both cgroups\sphinxhyphen{}v1 and cgroups\sphinxhyphen{}v2 io controllers, namely
blkio and io. In particular, BFQ supports weight\sphinxhyphen{}based proportional
share. To activate cgroups support, set BFQ\_GROUP\_IOSCHED.


\subsection{4\sphinxhyphen{}1 Service guarantees provided}
\label{\detokenize{bfq-iosched:service-guarantees-provided}}
With BFQ, proportional share means true proportional share of the
device bandwidth, according to group weights. For example, a group
with weight 200 gets twice the bandwidth, and not just twice the time,
of a group with weight 100.

BFQ supports hierarchies (group trees) of any depth. Bandwidth is
distributed among groups and processes in the expected way: for each
group, the children of the group share the whole bandwidth of the
group in proportion to their weights. In particular, this implies
that, for each leaf group, every process of the group receives the
same share of the whole group bandwidth, unless the ioprio of the
process is modified.

The resource\sphinxhyphen{}sharing guarantee for a group may partially or totally
switch from bandwidth to time, if providing bandwidth guarantees to
the group lowers the throughput too much. This switch occurs on a
per\sphinxhyphen{}process basis: if a process of a leaf group causes throughput loss
if served in such a way to receive its share of the bandwidth, then
BFQ switches back to just time\sphinxhyphen{}based proportional share for that
process.


\subsection{4\sphinxhyphen{}2 Interface}
\label{\detokenize{bfq-iosched:interface}}
To get proportional sharing of bandwidth with BFQ for a given device,
BFQ must of course be the active scheduler for that device.

Within each group directory, the names of the files associated with
BFQ\sphinxhyphen{}specific cgroup parameters and stats begin with the "bfq."
prefix. So, with cgroups\sphinxhyphen{}v1 or cgroups\sphinxhyphen{}v2, the full prefix for
BFQ\sphinxhyphen{}specific files is "blkio.bfq." or "io.bfq." For example, the group
parameter to set the weight of a group with BFQ is blkio.bfq.weight
or io.bfq.weight.

As for cgroups\sphinxhyphen{}v1 (blkio controller), the exact set of stat files
created, and kept up\sphinxhyphen{}to\sphinxhyphen{}date by bfq, depends on whether
CONFIG\_BFQ\_CGROUP\_DEBUG is set. If it is set, then bfq creates all
the stat files documented in
Documentation/admin\sphinxhyphen{}guide/cgroup\sphinxhyphen{}v1/blkio\sphinxhyphen{}controller.rst. If, instead,
CONFIG\_BFQ\_CGROUP\_DEBUG is not set, then bfq creates only the files:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
blkio.bfq.io\PYGZus{}service\PYGZus{}bytes
blkio.bfq.io\PYGZus{}service\PYGZus{}bytes\PYGZus{}recursive
blkio.bfq.io\PYGZus{}serviced
blkio.bfq.io\PYGZus{}serviced\PYGZus{}recursive
\end{sphinxVerbatim}

The value of CONFIG\_BFQ\_CGROUP\_DEBUG greatly influences the maximum
throughput sustainable with bfq, because updating the blkio.bfq.*
stats is rather costly, especially for some of the stats enabled by
CONFIG\_BFQ\_CGROUP\_DEBUG.


\subsection{Parameters}
\label{\detokenize{bfq-iosched:parameters}}
For each group, the following parameters can be set:
\begin{quote}
\begin{description}
\item[{weight}] \leavevmode
This specifies the default weight for the cgroup inside its parent.
Available values: 1..1000 (default: 100).

For cgroup v1, it is set by writing the value to \sphinxtitleref{blkio.bfq.weight}.

For cgroup v2, it is set by writing the value to \sphinxtitleref{io.bfq.weight}.
(with an optional prefix of \sphinxtitleref{default} and a space).

The linear mapping between ioprio and weights, described at the beginning
of the tunable section, is still valid, but all weights higher than
IOPRIO\_BE\_NR*10 are mapped to ioprio 0.

Recall that, if low\sphinxhyphen{}latency is set, then BFQ automatically raises the
weight of the queues associated with interactive and soft real\sphinxhyphen{}time
applications. Unset this tunable if you need/want to control weights.

\item[{weight\_device}] \leavevmode
This specifies a per\sphinxhyphen{}device weight for the cgroup. The syntax is
\sphinxtitleref{minor:major weight}. A weight of \sphinxtitleref{0} may be used to reset to the default
weight.

For cgroup v1, it is set by writing the value to \sphinxtitleref{blkio.bfq.weight\_device}.

For cgroup v2, the file name is \sphinxtitleref{io.bfq.weight}.

\end{description}
\end{quote}
\begin{description}
\item[{{[}1{]}}] \leavevmode
P. Valente, A. Avanzini, "Evolution of the BFQ Storage I/O
Scheduler", Proceedings of the First Workshop on Mobile System
Technologies (MST\sphinxhyphen{}2015), May 2015.

\sphinxurl{http://algogroup.unimore.it/people/paolo/disk\_sched/mst-2015.pdf}

\item[{{[}2{]}}] \leavevmode
P. Valente and M. Andreolini, "Improving Application
Responsiveness with the BFQ Disk I/O Scheduler", Proceedings of
the 5th Annual International Systems and Storage Conference
(SYSTOR \textquotesingle{}12), June 2012.

Slightly extended version:

\sphinxurl{http://algogroup.unimore.it/people/paolo/disk\_sched/bfq-v1-suite-results.pdf}

\item[{{[}3{]}}] \leavevmode
\sphinxurl{https://github.com/Algodev-github/S}

\end{description}


\chapter{Immutable biovecs and biovec iterators}
\label{\detokenize{biovecs:immutable-biovecs-and-biovec-iterators}}\label{\detokenize{biovecs::doc}}
Kent Overstreet \textless{}\sphinxhref{mailto:kmo@daterainc.com}{kmo@daterainc.com}\textgreater{}

As of 3.13, biovecs should never be modified after a bio has been submitted.
Instead, we have a new struct bvec\_iter which represents a range of a biovec \sphinxhyphen{}
the iterator will be modified as the bio is completed, not the biovec.

More specifically, old code that needed to partially complete a bio would
update bi\_sector and bi\_size, and advance bi\_idx to the next biovec. If it
ended up partway through a biovec, it would increment bv\_offset and decrement
bv\_len by the number of bytes completed in that biovec.

In the new scheme of things, everything that must be mutated in order to
partially complete a bio is segregated into struct bvec\_iter: bi\_sector,
bi\_size and bi\_idx have been moved there; and instead of modifying bv\_offset
and bv\_len, struct bvec\_iter has bi\_bvec\_done, which represents the number of
bytes completed in the current bvec.

There are a bunch of new helper macros for hiding the gory details \sphinxhyphen{} in
particular, presenting the illusion of partially completed biovecs so that
normal code doesn\textquotesingle{}t have to deal with bi\_bvec\_done.
\begin{itemize}
\item {} 
Driver code should no longer refer to biovecs directly; we now have
bio\_iovec() and bio\_iter\_iovec() macros that return literal struct biovecs,
constructed from the raw biovecs but taking into account bi\_bvec\_done and
bi\_size.

bio\_for\_each\_segment() has been updated to take a bvec\_iter argument
instead of an integer (that corresponded to bi\_idx); for a lot of code the
conversion just required changing the types of the arguments to
bio\_for\_each\_segment().

\item {} 
Advancing a bvec\_iter is done with bio\_advance\_iter(); bio\_advance() is a
wrapper around bio\_advance\_iter() that operates on bio\sphinxhyphen{}\textgreater{}bi\_iter, and also
advances the bio integrity\textquotesingle{}s iter if present.

There is a lower level advance function \sphinxhyphen{} bvec\_iter\_advance() \sphinxhyphen{} which takes
a pointer to a biovec, not a bio; this is used by the bio integrity code.

\end{itemize}

As of 5.12 bvec segments with zero bv\_len are not supported.


\section{What\textquotesingle{}s all this get us?}
\label{\detokenize{biovecs:what-s-all-this-get-us}}
Having a real iterator, and making biovecs immutable, has a number of
advantages:
\begin{itemize}
\item {} 
Before, iterating over bios was very awkward when you weren\textquotesingle{}t processing
exactly one bvec at a time \sphinxhyphen{} for example, bio\_copy\_data() in block/bio.c,
which copies the contents of one bio into another. Because the biovecs
wouldn\textquotesingle{}t necessarily be the same size, the old code was tricky convoluted \sphinxhyphen{}
it had to walk two different bios at the same time, keeping both bi\_idx and
and offset into the current biovec for each.

The new code is much more straightforward \sphinxhyphen{} have a look. This sort of
pattern comes up in a lot of places; a lot of drivers were essentially open
coding bvec iterators before, and having common implementation considerably
simplifies a lot of code.

\item {} 
Before, any code that might need to use the biovec after the bio had been
completed (perhaps to copy the data somewhere else, or perhaps to resubmit
it somewhere else if there was an error) had to save the entire bvec array
\sphinxhyphen{} again, this was being done in a fair number of places.

\item {} 
Biovecs can be shared between multiple bios \sphinxhyphen{} a bvec iter can represent an
arbitrary range of an existing biovec, both starting and ending midway
through biovecs. This is what enables efficient splitting of arbitrary
bios. Note that this means we \_only\_ use bi\_size to determine when we\textquotesingle{}ve
reached the end of a bio, not bi\_vcnt \sphinxhyphen{} and the bio\_iovec() macro takes
bi\_size into account when constructing biovecs.

\item {} 
Splitting bios is now much simpler. The old bio\_split() didn\textquotesingle{}t even work on
bios with more than a single bvec! Now, we can efficiently split arbitrary
size bios \sphinxhyphen{} because the new bio can share the old bio\textquotesingle{}s biovec.

Care must be taken to ensure the biovec isn\textquotesingle{}t freed while the split bio is
still using it, in case the original bio completes first, though. Using
bio\_chain() when splitting bios helps with this.

\item {} 
Submitting partially completed bios is now perfectly fine \sphinxhyphen{} this comes up
occasionally in stacking block drivers and various code (e.g. md and
bcache) had some ugly workarounds for this.

It used to be the case that submitting a partially completed bio would work
fine to \_most\_ devices, but since accessing the raw bvec array was the
norm, not all drivers would respect bi\_idx and those would break. Now,
since all drivers \_must\_ go through the bvec iterator \sphinxhyphen{} and have been
audited to make sure they are \sphinxhyphen{} submitting partially completed bios is
perfectly fine.

\end{itemize}


\section{Other implications:}
\label{\detokenize{biovecs:other-implications}}\begin{itemize}
\item {} 
Almost all usage of bi\_idx is now incorrect and has been removed; instead,
where previously you would have used bi\_idx you\textquotesingle{}d now use a bvec\_iter,
probably passing it to one of the helper macros.

I.e. instead of using bio\_iovec\_idx() (or bio\sphinxhyphen{}\textgreater{}bi\_iovec{[}bio\sphinxhyphen{}\textgreater{}bi\_idx{]}), you
now use bio\_iter\_iovec(), which takes a bvec\_iter and returns a
literal struct bio\_vec \sphinxhyphen{} constructed on the fly from the raw biovec but
taking into account bi\_bvec\_done (and bi\_size).

\item {} 
bi\_vcnt can\textquotesingle{}t be trusted or relied upon by driver code \sphinxhyphen{} i.e. anything that
doesn\textquotesingle{}t actually own the bio. The reason is twofold: firstly, it\textquotesingle{}s not
actually needed for iterating over the bio anymore \sphinxhyphen{} we only use bi\_size.
Secondly, when cloning a bio and reusing (a portion of) the original bio\textquotesingle{}s
biovec, in order to calculate bi\_vcnt for the new bio we\textquotesingle{}d have to iterate
over all the biovecs in the new bio \sphinxhyphen{} which is silly as it\textquotesingle{}s not needed.

So, don\textquotesingle{}t use bi\_vcnt anymore.

\item {} 
The current interface allows the block layer to split bios as needed, so we
could eliminate a lot of complexity particularly in stacked drivers. Code
that creates bios can then create whatever size bios are convenient, and
more importantly stacked drivers don\textquotesingle{}t have to deal with both their own bio
size limitations and the limitations of the underlying devices. Thus
there\textquotesingle{}s no need to define \sphinxhyphen{}\textgreater{}merge\_bvec\_fn() callbacks for individual block
drivers.

\end{itemize}


\section{Usage of helpers:}
\label{\detokenize{biovecs:usage-of-helpers}}\begin{itemize}
\item {} 
The following helpers whose names have the suffix of \sphinxtitleref{\_all} can only be used
on non\sphinxhyphen{}BIO\_CLONED bio. They are usually used by filesystem code. Drivers
shouldn\textquotesingle{}t use them because the bio may have been split before it reached the
driver.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
bio\PYGZus{}for\PYGZus{}each\PYGZus{}segment\PYGZus{}all()
bio\PYGZus{}for\PYGZus{}each\PYGZus{}bvec\PYGZus{}all()
bio\PYGZus{}first\PYGZus{}bvec\PYGZus{}all()
bio\PYGZus{}first\PYGZus{}page\PYGZus{}all()
bio\PYGZus{}last\PYGZus{}bvec\PYGZus{}all()
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
The following helpers iterate over single\sphinxhyphen{}page segment. The passed \textquotesingle{}struct
bio\_vec\textquotesingle{} will contain a single\sphinxhyphen{}page IO vector during the iteration:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
bio\PYGZus{}for\PYGZus{}each\PYGZus{}segment()
bio\PYGZus{}for\PYGZus{}each\PYGZus{}segment\PYGZus{}all()
\end{sphinxVerbatim}

\item {} 
The following helpers iterate over multi\sphinxhyphen{}page bvec. The passed \textquotesingle{}struct
bio\_vec\textquotesingle{} will contain a multi\sphinxhyphen{}page IO vector during the iteration:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
bio\PYGZus{}for\PYGZus{}each\PYGZus{}bvec()
bio\PYGZus{}for\PYGZus{}each\PYGZus{}bvec\PYGZus{}all()
rq\PYGZus{}for\PYGZus{}each\PYGZus{}bvec()
\end{sphinxVerbatim}

\end{itemize}


\chapter{Multi\sphinxhyphen{}Queue Block IO Queueing Mechanism (blk\sphinxhyphen{}mq)}
\label{\detokenize{blk-mq:multi-queue-block-io-queueing-mechanism-blk-mq}}\label{\detokenize{blk-mq::doc}}
The Multi\sphinxhyphen{}Queue Block IO Queueing Mechanism is an API to enable fast storage
devices to achieve a huge number of input/output operations per second (IOPS)
through queueing and submitting IO requests to block devices simultaneously,
benefiting from the parallelism offered by modern storage devices.


\section{Introduction}
\label{\detokenize{blk-mq:introduction}}

\subsection{Background}
\label{\detokenize{blk-mq:background}}
Magnetic hard disks have been the de facto standard from the beginning of the
development of the kernel. The Block IO subsystem aimed to achieve the best
performance possible for those devices with a high penalty when doing random
access, and the bottleneck was the mechanical moving parts, a lot slower than
any layer on the storage stack. One example of such optimization technique
involves ordering read/write requests according to the current position of the
hard disk head.

However, with the development of Solid State Drives and Non\sphinxhyphen{}Volatile Memories
without mechanical parts nor random access penalty and capable of performing
high parallel access, the bottleneck of the stack had moved from the storage
device to the operating system. In order to take advantage of the parallelism
in those devices\textquotesingle{} design, the multi\sphinxhyphen{}queue mechanism was introduced.

The former design had a single queue to store block IO requests with a single
lock. That did not scale well in SMP systems due to dirty data in cache and the
bottleneck of having a single lock for multiple processors. This setup also
suffered with congestion when different processes (or the same process, moving
to different CPUs) wanted to perform block IO. Instead of this, the blk\sphinxhyphen{}mq API
spawns multiple queues with individual entry points local to the CPU, removing
the need for a lock. A deeper explanation on how this works is covered in the
following section ({\hyperref[\detokenize{blk-mq:operation}]{\sphinxcrossref{Operation}}}).


\subsection{Operation}
\label{\detokenize{blk-mq:operation}}
When the userspace performs IO to a block device (reading or writing a file,
for instance), blk\sphinxhyphen{}mq takes action: it will store and manage IO requests to
the block device, acting as middleware between the userspace (and a file
system, if present) and the block device driver.

blk\sphinxhyphen{}mq has two group of queues: software staging queues and hardware dispatch
queues. When the request arrives at the block layer, it will try the shortest
path possible: send it directly to the hardware queue. However, there are two
cases that it might not do that: if there\textquotesingle{}s an IO scheduler attached at the
layer or if we want to try to merge requests. In both cases, requests will be
sent to the software queue.

Then, after the requests are processed by software queues, they will be placed
at the hardware queue, a second stage queue where the hardware has direct access
to process those requests. However, if the hardware does not have enough
resources to accept more requests, blk\sphinxhyphen{}mq will places requests on a temporary
queue, to be sent in the future, when the hardware is able.


\subsubsection{Software staging queues}
\label{\detokenize{blk-mq:software-staging-queues}}
The block IO subsystem adds requests in the software staging queues
(represented by struct blk\_mq\_ctx) in case that they weren\textquotesingle{}t sent
directly to the driver. A request is one or more BIOs. They arrived at the
block layer through the data structure struct bio. The block layer
will then build a new structure from it, the struct request that will
be used to communicate with the device driver. Each queue has its own lock and
the number of queues is defined by a per\sphinxhyphen{}CPU or per\sphinxhyphen{}node basis.

The staging queue can be used to merge requests for adjacent sectors. For
instance, requests for sector 3\sphinxhyphen{}6, 6\sphinxhyphen{}7, 7\sphinxhyphen{}9 can become one request for 3\sphinxhyphen{}9.
Even if random access to SSDs and NVMs have the same time of response compared
to sequential access, grouped requests for sequential access decreases the
number of individual requests. This technique of merging requests is called
plugging.

Along with that, the requests can be reordered to ensure fairness of system
resources (e.g. to ensure that no application suffers from starvation) and/or to
improve IO performance, by an IO scheduler.


\paragraph{IO Schedulers}
\label{\detokenize{blk-mq:io-schedulers}}
There are several schedulers implemented by the block layer, each one following
a heuristic to improve the IO performance. They are "pluggable" (as in plug
and play), in the sense of they can be selected at run time using sysfs. You
can read more about Linux\textquotesingle{}s IO schedulers \sphinxhref{https://www.kernel.org/doc/html/latest/block/index.html}{here}. The scheduling
happens only between requests in the same queue, so it is not possible to merge
requests from different queues, otherwise there would be cache trashing and a
need to have a lock for each queue. After the scheduling, the requests are
eligible to be sent to the hardware. One of the possible schedulers to be
selected is the NONE scheduler, the most straightforward one. It will just
place requests on whatever software queue the process is running on, without
any reordering. When the device starts processing requests in the hardware
queue (a.k.a. run the hardware queue), the software queues mapped to that
hardware queue will be drained in sequence according to their mapping.


\subsubsection{Hardware dispatch queues}
\label{\detokenize{blk-mq:hardware-dispatch-queues}}
The hardware queue (represented by {\hyperref[\detokenize{blk-mq:c.blk_mq_hw_ctx}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{struct blk\_mq\_hw\_ctx}}}}}) is a struct
used by device drivers to map the device submission queues (or device DMA ring
buffer), and are the last step of the block layer submission code before the
low level device driver taking ownership of the request. To run this queue, the
block layer removes requests from the associated software queues and tries to
dispatch to the hardware.

If it\textquotesingle{}s not possible to send the requests directly to hardware, they will be
added to a linked list (\sphinxcode{\sphinxupquote{hctx\sphinxhyphen{}\textgreater{}dispatch}}) of requests. Then,
next time the block layer runs a queue, it will send the requests laying at the
\sphinxcode{\sphinxupquote{dispatch}} list first, to ensure a fairness dispatch with those
requests that were ready to be sent first. The number of hardware queues
depends on the number of hardware contexts supported by the hardware and its
device driver, but it will not be more than the number of cores of the system.
There is no reordering at this stage, and each software queue has a set of
hardware queues to send requests for.

\begin{sphinxadmonition}{note}{Note:}
Neither the block layer nor the device protocols guarantee
the order of completion of requests. This must be handled by
higher layers, like the filesystem.
\end{sphinxadmonition}


\subsubsection{Tag\sphinxhyphen{}based completion}
\label{\detokenize{blk-mq:tag-based-completion}}
In order to indicate which request has been completed, every request is
identified by an integer, ranging from 0 to the dispatch queue size. This tag
is generated by the block layer and later reused by the device driver, removing
the need to create a redundant identifier. When a request is completed in the
driver, the tag is sent back to the block layer to notify it of the finalization.
This removes the need to do a linear search to find out which IO has been
completed.


\subsection{Further reading}
\label{\detokenize{blk-mq:further-reading}}\begin{itemize}
\item {} 
\sphinxhref{http://kernel.dk/blk-mq.pdf}{Linux Block IO: Introducing Multi\sphinxhyphen{}queue SSD Access on Multi\sphinxhyphen{}core Systems}

\item {} 
\sphinxhref{https://en.wikipedia.org/wiki/Noop\_scheduler}{NOOP scheduler}

\item {} 
\sphinxhref{https://www.kernel.org/doc/html/latest/block/null\_blk.html}{Null block device driver}

\end{itemize}


\section{Source code documentation}
\label{\detokenize{blk-mq:source-code-documentation}}\index{rq\_list\_move (C function)@\spxentry{rq\_list\_move}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.rq_list_move}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{rq\_list\_move}}}{struct request\sphinxstyleemphasis{ **src}, struct request\sphinxstyleemphasis{ **dst}, struct request\sphinxstyleemphasis{ *rq}, struct request\sphinxstyleemphasis{ *prev}}{}
move a struct request from one list to another

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request **src}}}] \leavevmode
The source list \sphinxstylestrong{rq} is currently in

\item[{\sphinxcode{\sphinxupquote{struct request **dst}}}] \leavevmode
The destination list that \sphinxstylestrong{rq} will be appended to

\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
The request to move

\item[{\sphinxcode{\sphinxupquote{struct request *prev}}}] \leavevmode
The request preceding \sphinxstylestrong{rq} in \sphinxstylestrong{src} (NULL if \sphinxstylestrong{rq} is the head)

\end{description}
\index{blk\_eh\_timer\_return (C type)@\spxentry{blk\_eh\_timer\_return}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_eh_timer_return}}\pysigline{enum \sphinxbfcode{\sphinxupquote{blk\_eh\_timer\_return}}}
How the timeout handler should proceed

\end{fulllineitems}


\sphinxstylestrong{Constants}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{BLK\_EH\_DONE}}}] \leavevmode
The block driver completed the command or will complete it at
a later time.

\item[{\sphinxcode{\sphinxupquote{BLK\_EH\_RESET\_TIMER}}}] \leavevmode
Reset the request timer and continue waiting for the
request to complete.

\end{description}
\index{blk\_mq\_hw\_ctx (C type)@\spxentry{blk\_mq\_hw\_ctx}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_hw_ctx}}\pysigline{struct \sphinxbfcode{\sphinxupquote{blk\_mq\_hw\_ctx}}}
State for a hardware queue facing the hardware block device

\end{fulllineitems}


\sphinxstylestrong{Definition}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
struct blk\PYGZus{}mq\PYGZus{}hw\PYGZus{}ctx \PYGZob{}
    struct \PYGZob{}
        spinlock\PYGZus{}t lock;
        struct list\PYGZus{}head        dispatch;
        unsigned long           state;
    \PYGZcb{};
    struct delayed\PYGZus{}work     run\PYGZus{}work;
    cpumask\PYGZus{}var\PYGZus{}t cpumask;
    int next\PYGZus{}cpu;
    int next\PYGZus{}cpu\PYGZus{}batch;
    unsigned long           flags;
    void *sched\PYGZus{}data;
    struct request\PYGZus{}queue    *queue;
    struct blk\PYGZus{}flush\PYGZus{}queue  *fq;
    void *driver\PYGZus{}data;
    struct sbitmap          ctx\PYGZus{}map;
    struct blk\PYGZus{}mq\PYGZus{}ctx       *dispatch\PYGZus{}from;
    unsigned int            dispatch\PYGZus{}busy;
    unsigned short          type;
    unsigned short          nr\PYGZus{}ctx;
    struct blk\PYGZus{}mq\PYGZus{}ctx       **ctxs;
    spinlock\PYGZus{}t dispatch\PYGZus{}wait\PYGZus{}lock;
    wait\PYGZus{}queue\PYGZus{}entry\PYGZus{}t dispatch\PYGZus{}wait;
    atomic\PYGZus{}t wait\PYGZus{}index;
    struct blk\PYGZus{}mq\PYGZus{}tags      *tags;
    struct blk\PYGZus{}mq\PYGZus{}tags      *sched\PYGZus{}tags;
    unsigned long           queued;
    unsigned long           run;
    unsigned int            numa\PYGZus{}node;
    unsigned int            queue\PYGZus{}num;
    atomic\PYGZus{}t nr\PYGZus{}active;
    struct hlist\PYGZus{}node       cpuhp\PYGZus{}online;
    struct hlist\PYGZus{}node       cpuhp\PYGZus{}dead;
    struct kobject          kobj;
\PYGZsh{}ifdef CONFIG\PYGZus{}BLK\PYGZus{}DEBUG\PYGZus{}FS;
    struct dentry           *debugfs\PYGZus{}dir;
    struct dentry           *sched\PYGZus{}debugfs\PYGZus{}dir;
\PYGZsh{}endif;
    struct list\PYGZus{}head        hctx\PYGZus{}list;
\PYGZcb{};
\end{sphinxVerbatim}

\sphinxstylestrong{Members}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{\{unnamed\_struct\}}}}] \leavevmode
anonymous

\item[{\sphinxcode{\sphinxupquote{lock}}}] \leavevmode
Protects the dispatch list.

\item[{\sphinxcode{\sphinxupquote{dispatch}}}] \leavevmode
Used for requests that are ready to be
dispatched to the hardware but for some reason (e.g. lack of
resources) could not be sent to the hardware. As soon as the
driver can send new requests, requests at this list will
be sent first for a fairer dispatch.

\item[{\sphinxcode{\sphinxupquote{state}}}] \leavevmode
BLK\_MQ\_S\_* flags. Defines the state of the hw
queue (active, scheduled to restart, stopped).

\item[{\sphinxcode{\sphinxupquote{run\_work}}}] \leavevmode
Used for scheduling a hardware queue run at a later time.

\item[{\sphinxcode{\sphinxupquote{cpumask}}}] \leavevmode
Map of available CPUs where this hctx can run.

\item[{\sphinxcode{\sphinxupquote{next\_cpu}}}] \leavevmode
Used by blk\_mq\_hctx\_next\_cpu() for round\sphinxhyphen{}robin CPU
selection from \sphinxstylestrong{cpumask}.

\item[{\sphinxcode{\sphinxupquote{next\_cpu\_batch}}}] \leavevmode
Counter of how many works left in the batch before
changing to the next CPU.

\item[{\sphinxcode{\sphinxupquote{flags}}}] \leavevmode
BLK\_MQ\_F\_* flags. Defines the behaviour of the queue.

\item[{\sphinxcode{\sphinxupquote{sched\_data}}}] \leavevmode
Pointer owned by the IO scheduler attached to a request
queue. It\textquotesingle{}s up to the IO scheduler how to use this pointer.

\item[{\sphinxcode{\sphinxupquote{queue}}}] \leavevmode
Pointer to the request queue that owns this hardware context.

\item[{\sphinxcode{\sphinxupquote{fq}}}] \leavevmode
Queue of requests that need to perform a flush operation.

\item[{\sphinxcode{\sphinxupquote{driver\_data}}}] \leavevmode
Pointer to data owned by the block driver that created
this hctx

\item[{\sphinxcode{\sphinxupquote{ctx\_map}}}] \leavevmode
Bitmap for each software queue. If bit is on, there is a
pending request in that software queue.

\item[{\sphinxcode{\sphinxupquote{dispatch\_from}}}] \leavevmode
Software queue to be used when no scheduler was
selected.

\item[{\sphinxcode{\sphinxupquote{dispatch\_busy}}}] \leavevmode
Number used by blk\_mq\_update\_dispatch\_busy() to
decide if the hw\_queue is busy using Exponential Weighted Moving
Average algorithm.

\item[{\sphinxcode{\sphinxupquote{type}}}] \leavevmode
HCTX\_TYPE\_* flags. Type of hardware queue.

\item[{\sphinxcode{\sphinxupquote{nr\_ctx}}}] \leavevmode
Number of software queues.

\item[{\sphinxcode{\sphinxupquote{ctxs}}}] \leavevmode
Array of software queues.

\item[{\sphinxcode{\sphinxupquote{dispatch\_wait\_lock}}}] \leavevmode
Lock for dispatch\_wait queue.

\item[{\sphinxcode{\sphinxupquote{dispatch\_wait}}}] \leavevmode
Waitqueue to put requests when there is no tag
available at the moment, to wait for another try in the future.

\item[{\sphinxcode{\sphinxupquote{wait\_index}}}] \leavevmode
Index of next available dispatch\_wait queue to insert
requests.

\item[{\sphinxcode{\sphinxupquote{tags}}}] \leavevmode
Tags owned by the block driver. A tag at this set is only
assigned when a request is dispatched from a hardware queue.

\item[{\sphinxcode{\sphinxupquote{sched\_tags}}}] \leavevmode
Tags owned by I/O scheduler. If there is an I/O
scheduler associated with a request queue, a tag is assigned when
that request is allocated. Else, this member is not used.

\item[{\sphinxcode{\sphinxupquote{queued}}}] \leavevmode
Number of queued requests.

\item[{\sphinxcode{\sphinxupquote{run}}}] \leavevmode
Number of dispatched requests.

\item[{\sphinxcode{\sphinxupquote{numa\_node}}}] \leavevmode
NUMA node the storage adapter has been connected to.

\item[{\sphinxcode{\sphinxupquote{queue\_num}}}] \leavevmode
Index of this hardware queue.

\item[{\sphinxcode{\sphinxupquote{nr\_active}}}] \leavevmode
Number of active requests. Only used when a tag set is
shared across request queues.

\item[{\sphinxcode{\sphinxupquote{cpuhp\_online}}}] \leavevmode
List to store request if CPU is going to die

\item[{\sphinxcode{\sphinxupquote{cpuhp\_dead}}}] \leavevmode
List to store request if some CPU die.

\item[{\sphinxcode{\sphinxupquote{kobj}}}] \leavevmode
Kernel object for sysfs.

\item[{\sphinxcode{\sphinxupquote{debugfs\_dir}}}] \leavevmode
debugfs directory for this hardware queue. Named
as cpu\textless{}cpu\_number\textgreater{}.

\item[{\sphinxcode{\sphinxupquote{sched\_debugfs\_dir}}}] \leavevmode
debugfs directory for the scheduler.

\item[{\sphinxcode{\sphinxupquote{hctx\_list}}}] \leavevmode
if this hctx is not in use, this is an entry in
q\sphinxhyphen{}\textgreater{}unused\_hctx\_list.

\end{description}
\index{blk\_mq\_queue\_map (C type)@\spxentry{blk\_mq\_queue\_map}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_queue_map}}\pysigline{struct \sphinxbfcode{\sphinxupquote{blk\_mq\_queue\_map}}}
Map software queues to hardware queues

\end{fulllineitems}


\sphinxstylestrong{Definition}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
struct blk\PYGZus{}mq\PYGZus{}queue\PYGZus{}map \PYGZob{}
    unsigned int *mq\PYGZus{}map;
    unsigned int nr\PYGZus{}queues;
    unsigned int queue\PYGZus{}offset;
\PYGZcb{};
\end{sphinxVerbatim}

\sphinxstylestrong{Members}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{mq\_map}}}] \leavevmode
CPU ID to hardware queue index map. This is an array
with nr\_cpu\_ids elements. Each element has a value in the range
{[}\sphinxstylestrong{queue\_offset}, \sphinxstylestrong{queue\_offset} + \sphinxstylestrong{nr\_queues}).

\item[{\sphinxcode{\sphinxupquote{nr\_queues}}}] \leavevmode
Number of hardware queues to map CPU IDs onto.

\item[{\sphinxcode{\sphinxupquote{queue\_offset}}}] \leavevmode
First hardware queue to map onto. Used by the PCIe NVMe
driver to map each hardware queue type ({\hyperref[\detokenize{blk-mq:c.hctx_type}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{enum hctx\_type}}}}}) onto a distinct
set of hardware queues.

\end{description}
\index{hctx\_type (C type)@\spxentry{hctx\_type}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.hctx_type}}\pysigline{enum \sphinxbfcode{\sphinxupquote{hctx\_type}}}
Type of hardware queue

\end{fulllineitems}


\sphinxstylestrong{Constants}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{HCTX\_TYPE\_DEFAULT}}}] \leavevmode
All I/O not otherwise accounted for.

\item[{\sphinxcode{\sphinxupquote{HCTX\_TYPE\_READ}}}] \leavevmode
Just for READ I/O.

\item[{\sphinxcode{\sphinxupquote{HCTX\_TYPE\_POLL}}}] \leavevmode
Polled I/O of any kind.

\item[{\sphinxcode{\sphinxupquote{HCTX\_MAX\_TYPES}}}] \leavevmode
Number of types of hctx.

\end{description}
\index{blk\_mq\_tag\_set (C type)@\spxentry{blk\_mq\_tag\_set}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_tag_set}}\pysigline{struct \sphinxbfcode{\sphinxupquote{blk\_mq\_tag\_set}}}
tag set that can be shared between request queues

\end{fulllineitems}


\sphinxstylestrong{Definition}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
struct blk\PYGZus{}mq\PYGZus{}tag\PYGZus{}set \PYGZob{}
    const struct blk\PYGZus{}mq\PYGZus{}ops *ops;
    struct blk\PYGZus{}mq\PYGZus{}queue\PYGZus{}map map[HCTX\PYGZus{}MAX\PYGZus{}TYPES];
    unsigned int            nr\PYGZus{}maps;
    unsigned int            nr\PYGZus{}hw\PYGZus{}queues;
    unsigned int            queue\PYGZus{}depth;
    unsigned int            reserved\PYGZus{}tags;
    unsigned int            cmd\PYGZus{}size;
    int numa\PYGZus{}node;
    unsigned int            timeout;
    unsigned int            flags;
    void *driver\PYGZus{}data;
    struct blk\PYGZus{}mq\PYGZus{}tags      **tags;
    struct blk\PYGZus{}mq\PYGZus{}tags      *shared\PYGZus{}tags;
    struct mutex            tag\PYGZus{}list\PYGZus{}lock;
    struct list\PYGZus{}head        tag\PYGZus{}list;
    struct srcu\PYGZus{}struct      *srcu;
\PYGZcb{};
\end{sphinxVerbatim}

\sphinxstylestrong{Members}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{ops}}}] \leavevmode
Pointers to functions that implement block driver behavior.

\item[{\sphinxcode{\sphinxupquote{map}}}] \leavevmode
One or more ctx \sphinxhyphen{}\textgreater{} hctx mappings. One map exists for each
hardware queue type ({\hyperref[\detokenize{blk-mq:c.hctx_type}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{enum hctx\_type}}}}}) that the driver wishes
to support. There are no restrictions on maps being of the
same size, and it\textquotesingle{}s perfectly legal to share maps between
types.

\item[{\sphinxcode{\sphinxupquote{nr\_maps}}}] \leavevmode
Number of elements in the \sphinxstylestrong{map} array. A number in the range
{[}1, HCTX\_MAX\_TYPES{]}.

\item[{\sphinxcode{\sphinxupquote{nr\_hw\_queues}}}] \leavevmode
Number of hardware queues supported by the block driver that
owns this data structure.

\item[{\sphinxcode{\sphinxupquote{queue\_depth}}}] \leavevmode
Number of tags per hardware queue, reserved tags included.

\item[{\sphinxcode{\sphinxupquote{reserved\_tags}}}] \leavevmode
Number of tags to set aside for BLK\_MQ\_REQ\_RESERVED tag
allocations.

\item[{\sphinxcode{\sphinxupquote{cmd\_size}}}] \leavevmode
Number of additional bytes to allocate per request. The block
driver owns these additional bytes.

\item[{\sphinxcode{\sphinxupquote{numa\_node}}}] \leavevmode
NUMA node the storage adapter has been connected to.

\item[{\sphinxcode{\sphinxupquote{timeout}}}] \leavevmode
Request processing timeout in jiffies.

\item[{\sphinxcode{\sphinxupquote{flags}}}] \leavevmode
Zero or more BLK\_MQ\_F\_* flags.

\item[{\sphinxcode{\sphinxupquote{driver\_data}}}] \leavevmode
Pointer to data owned by the block driver that created this
tag set.

\item[{\sphinxcode{\sphinxupquote{tags}}}] \leavevmode
Tag sets. One tag set per hardware queue. Has \sphinxstylestrong{nr\_hw\_queues}
elements.

\end{description}

\sphinxcode{\sphinxupquote{shared\_tags}}
\begin{quote}

Shared set of tags. Has \sphinxstylestrong{nr\_hw\_queues} elements. If set,
shared by all \sphinxstylestrong{tags}.
\end{quote}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{tag\_list\_lock}}}] \leavevmode
Serializes tag\_list accesses.

\item[{\sphinxcode{\sphinxupquote{tag\_list}}}] \leavevmode
List of the request queues that use this tag set. See also
request\_queue.tag\_set\_list.

\item[{\sphinxcode{\sphinxupquote{srcu}}}] \leavevmode
Use as lock when type of the request queue is blocking
(BLK\_MQ\_F\_BLOCKING).

\end{description}
\index{blk\_mq\_queue\_data (C type)@\spxentry{blk\_mq\_queue\_data}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_queue_data}}\pysigline{struct \sphinxbfcode{\sphinxupquote{blk\_mq\_queue\_data}}}
Data about a request inserted in a queue

\end{fulllineitems}


\sphinxstylestrong{Definition}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
struct blk\PYGZus{}mq\PYGZus{}queue\PYGZus{}data \PYGZob{}
    struct request *rq;
    bool last;
\PYGZcb{};
\end{sphinxVerbatim}

\sphinxstylestrong{Members}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{rq}}}] \leavevmode
Request pointer.

\item[{\sphinxcode{\sphinxupquote{last}}}] \leavevmode
If it is the last request in the queue.

\end{description}
\index{blk\_mq\_ops (C type)@\spxentry{blk\_mq\_ops}\spxextra{C type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_ops}}\pysigline{struct \sphinxbfcode{\sphinxupquote{blk\_mq\_ops}}}
Callback functions that implements block driver behaviour.

\end{fulllineitems}


\sphinxstylestrong{Definition}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
struct blk\PYGZus{}mq\PYGZus{}ops \PYGZob{}
    blk\PYGZus{}status\PYGZus{}t (*queue\PYGZus{}rq)(struct blk\PYGZus{}mq\PYGZus{}hw\PYGZus{}ctx *, const struct blk\PYGZus{}mq\PYGZus{}queue\PYGZus{}data *);
    void (*commit\PYGZus{}rqs)(struct blk\PYGZus{}mq\PYGZus{}hw\PYGZus{}ctx *);
    void (*queue\PYGZus{}rqs)(struct request **rqlist);
    int (*get\PYGZus{}budget)(struct request\PYGZus{}queue *);
    void (*put\PYGZus{}budget)(struct request\PYGZus{}queue *, int);
    void (*set\PYGZus{}rq\PYGZus{}budget\PYGZus{}token)(struct request *, int);
    int (*get\PYGZus{}rq\PYGZus{}budget\PYGZus{}token)(struct request *);
    enum blk\PYGZus{}eh\PYGZus{}timer\PYGZus{}return (*timeout)(struct request *);
    int (*poll)(struct blk\PYGZus{}mq\PYGZus{}hw\PYGZus{}ctx *, struct io\PYGZus{}comp\PYGZus{}batch *);
    void (*complete)(struct request *);
    int (*init\PYGZus{}hctx)(struct blk\PYGZus{}mq\PYGZus{}hw\PYGZus{}ctx *, void *, unsigned int);
    void (*exit\PYGZus{}hctx)(struct blk\PYGZus{}mq\PYGZus{}hw\PYGZus{}ctx *, unsigned int);
    int (*init\PYGZus{}request)(struct blk\PYGZus{}mq\PYGZus{}tag\PYGZus{}set *set, struct request *, unsigned int, unsigned int);
    void (*exit\PYGZus{}request)(struct blk\PYGZus{}mq\PYGZus{}tag\PYGZus{}set *set, struct request *, unsigned int);
    void (*cleanup\PYGZus{}rq)(struct request *);
    bool (*busy)(struct request\PYGZus{}queue *);
    void (*map\PYGZus{}queues)(struct blk\PYGZus{}mq\PYGZus{}tag\PYGZus{}set *set);
\PYGZsh{}ifdef CONFIG\PYGZus{}BLK\PYGZus{}DEBUG\PYGZus{}FS;
    void (*show\PYGZus{}rq)(struct seq\PYGZus{}file *m, struct request *rq);
\PYGZsh{}endif;
\PYGZcb{};
\end{sphinxVerbatim}

\sphinxstylestrong{Members}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{queue\_rq}}}] \leavevmode
Queue a new request from block IO.

\item[{\sphinxcode{\sphinxupquote{commit\_rqs}}}] \leavevmode
If a driver uses bd\sphinxhyphen{}\textgreater{}last to judge when to submit
requests to hardware, it must define this function. In case of errors
that make us stop issuing further requests, this hook serves the
purpose of kicking the hardware (which the last request otherwise
would have done).

\item[{\sphinxcode{\sphinxupquote{queue\_rqs}}}] \leavevmode
Queue a list of new requests. Driver is guaranteed
that each request belongs to the same queue. If the driver doesn\textquotesingle{}t
empty the \sphinxstylestrong{rqlist} completely, then the rest will be queued
individually by the block layer upon return.

\item[{\sphinxcode{\sphinxupquote{get\_budget}}}] \leavevmode
Reserve budget before queue request, once .queue\_rq is
run, it is driver\textquotesingle{}s responsibility to release the
reserved budget. Also we have to handle failure case
of .get\_budget for avoiding I/O deadlock.

\item[{\sphinxcode{\sphinxupquote{put\_budget}}}] \leavevmode
Release the reserved budget.

\item[{\sphinxcode{\sphinxupquote{set\_rq\_budget\_token}}}] \leavevmode
store rq\textquotesingle{}s budget token

\item[{\sphinxcode{\sphinxupquote{get\_rq\_budget\_token}}}] \leavevmode
retrieve rq\textquotesingle{}s budget token

\item[{\sphinxcode{\sphinxupquote{timeout}}}] \leavevmode
Called on request timeout.

\item[{\sphinxcode{\sphinxupquote{poll}}}] \leavevmode
Called to poll for completion of a specific tag.

\item[{\sphinxcode{\sphinxupquote{complete}}}] \leavevmode
Mark the request as complete.

\item[{\sphinxcode{\sphinxupquote{init\_hctx}}}] \leavevmode
Called when the block layer side of a hardware queue has
been set up, allowing the driver to allocate/init matching
structures.

\item[{\sphinxcode{\sphinxupquote{exit\_hctx}}}] \leavevmode
Ditto for exit/teardown.

\item[{\sphinxcode{\sphinxupquote{init\_request}}}] \leavevmode
Called for every command allocated by the block layer
to allow the driver to set up driver specific data.

Tag greater than or equal to queue\_depth is for setting up
flush request.

\item[{\sphinxcode{\sphinxupquote{exit\_request}}}] \leavevmode
Ditto for exit/teardown.

\item[{\sphinxcode{\sphinxupquote{cleanup\_rq}}}] \leavevmode
Called before freeing one request which isn\textquotesingle{}t completed
yet, and usually for freeing the driver private data.

\item[{\sphinxcode{\sphinxupquote{busy}}}] \leavevmode
If set, returns whether or not this queue currently is busy.

\item[{\sphinxcode{\sphinxupquote{map\_queues}}}] \leavevmode
This allows drivers specify their own queue mapping by
overriding the setup\sphinxhyphen{}time function that builds the mq\_map.

\item[{\sphinxcode{\sphinxupquote{show\_rq}}}] \leavevmode
Used by the debugfs implementation to show driver\sphinxhyphen{}specific
information about a request.

\end{description}
\index{blk\_mq\_rq\_state (C function)@\spxentry{blk\_mq\_rq\_state}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_rq_state}}\pysiglinewithargsret{enum mq\_rq\_state \sphinxbfcode{\sphinxupquote{blk\_mq\_rq\_state}}}{struct request\sphinxstyleemphasis{ *rq}}{}
read the current MQ\_RQ\_* state of a request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
target request.

\end{description}
\index{blk\_mq\_rq\_from\_pdu (C function)@\spxentry{blk\_mq\_rq\_from\_pdu}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_rq_from_pdu}}\pysiglinewithargsret{struct request * \sphinxbfcode{\sphinxupquote{blk\_mq\_rq\_from\_pdu}}}{void\sphinxstyleemphasis{ *pdu}}{}
cast a PDU to a request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{void *pdu}}}] \leavevmode
the PDU (Protocol Data Unit) to be casted

\end{description}

\sphinxstylestrong{Return}

request

\sphinxstylestrong{Description}

Driver command data is immediately after the request. So subtract request
size to get back to the original request.
\index{blk\_mq\_rq\_to\_pdu (C function)@\spxentry{blk\_mq\_rq\_to\_pdu}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_rq_to_pdu}}\pysiglinewithargsret{void * \sphinxbfcode{\sphinxupquote{blk\_mq\_rq\_to\_pdu}}}{struct request\sphinxstyleemphasis{ *rq}}{}
cast a request to a PDU

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
the request to be casted

\end{description}

\sphinxstylestrong{Return}

pointer to the PDU

\sphinxstylestrong{Description}

Driver command data is immediately after the request. So add request to get
the PDU.
\index{blk\_mq\_wait\_quiesce\_done (C function)@\spxentry{blk\_mq\_wait\_quiesce\_done}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_wait_quiesce_done}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_wait\_quiesce\_done}}}{struct {\hyperref[\detokenize{blk-mq:c.blk_mq_tag_set}]{\sphinxcrossref{blk\_mq\_tag\_set}}}\sphinxstyleemphasis{ *set}}{}
wait until in\sphinxhyphen{}progress quiesce is done

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct blk\_mq\_tag\_set *set}}}] \leavevmode
tag\_set to wait on

\end{description}

\sphinxstylestrong{Note}

it is driver\textquotesingle{}s responsibility for making sure that quiesce has
been started on or more of the request\_queues of the tag\_set.  This
function only waits for the quiesce on those request\_queues that had
the quiesce flag set using blk\_mq\_quiesce\_queue\_nowait.
\index{blk\_mq\_quiesce\_queue (C function)@\spxentry{blk\_mq\_quiesce\_queue}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_quiesce_queue}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_quiesce\_queue}}}{struct request\_queue\sphinxstyleemphasis{ *q}}{}
wait until all ongoing dispatches have finished

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request\_queue *q}}}] \leavevmode
request queue.

\end{description}

\sphinxstylestrong{Note}

this function does not prevent that the struct request end\_io()
callback function is invoked. Once this function is returned, we make
sure no dispatch can happen until the queue is unquiesced via
blk\_mq\_unquiesce\_queue().
\index{blk\_update\_request (C function)@\spxentry{blk\_update\_request}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_update_request}}\pysiglinewithargsret{bool \sphinxbfcode{\sphinxupquote{blk\_update\_request}}}{struct request\sphinxstyleemphasis{ *req}, blk\_status\_t\sphinxstyleemphasis{ error}, unsigned int\sphinxstyleemphasis{ nr\_bytes}}{}
Complete multiple bytes without completing the request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *req}}}] \leavevmode
the request being processed

\item[{\sphinxcode{\sphinxupquote{blk\_status\_t error}}}] \leavevmode
block status code

\item[{\sphinxcode{\sphinxupquote{unsigned int nr\_bytes}}}] \leavevmode
number of bytes to complete for \sphinxstylestrong{req}

\end{description}

\sphinxstylestrong{Description}
\begin{quote}

Ends I/O on a number of bytes attached to \sphinxstylestrong{req}, but doesn\textquotesingle{}t complete
the request structure even if \sphinxstylestrong{req} doesn\textquotesingle{}t have leftover.
If \sphinxstylestrong{req} has leftover, sets it up for the next range of segments.

Passing the result of blk\_rq\_bytes() as \sphinxstylestrong{nr\_bytes} guarantees
\sphinxcode{\sphinxupquote{false}} return from this function.
\end{quote}

\sphinxstylestrong{Note}
\begin{quote}

The RQF\_SPECIAL\_PAYLOAD flag is ignored on purpose in this function
except in the consistency check at the end of this function.
\end{quote}

\sphinxstylestrong{Return}
\begin{quote}

\sphinxcode{\sphinxupquote{false}} \sphinxhyphen{} this request doesn\textquotesingle{}t have any more data
\sphinxcode{\sphinxupquote{true}}  \sphinxhyphen{} this request has more data
\end{quote}
\index{blk\_mq\_complete\_request (C function)@\spxentry{blk\_mq\_complete\_request}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_complete_request}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_complete\_request}}}{struct request\sphinxstyleemphasis{ *rq}}{}
end I/O on a request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
the request being processed

\end{description}

\sphinxstylestrong{Description}
\begin{quote}

Complete a request by scheduling the \sphinxhyphen{}\textgreater{}complete\_rq operation.
\end{quote}
\index{blk\_mq\_start\_request (C function)@\spxentry{blk\_mq\_start\_request}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_start_request}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_start\_request}}}{struct request\sphinxstyleemphasis{ *rq}}{}
Start processing a request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
Pointer to request to be started

\end{description}

\sphinxstylestrong{Description}

Function used by device drivers to notify the block layer that a request
is going to be processed now, so blk layer can do proper initializations
such as starting the timeout timer.
\index{blk\_execute\_rq\_nowait (C function)@\spxentry{blk\_execute\_rq\_nowait}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_execute_rq_nowait}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_execute\_rq\_nowait}}}{struct request\sphinxstyleemphasis{ *rq}, bool\sphinxstyleemphasis{ at\_head}}{}
insert a request to I/O scheduler for execution

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
request to insert

\item[{\sphinxcode{\sphinxupquote{bool at\_head}}}] \leavevmode
insert request at head or tail of queue

\end{description}

\sphinxstylestrong{Description}
\begin{quote}

Insert a fully prepared request at the back of the I/O scheduler queue
for execution.  Don\textquotesingle{}t wait for completion.
\end{quote}

\sphinxstylestrong{Note}
\begin{quote}

This function will invoke \sphinxstylestrong{done} directly if the queue is dead.
\end{quote}
\index{blk\_execute\_rq (C function)@\spxentry{blk\_execute\_rq}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_execute_rq}}\pysiglinewithargsret{blk\_status\_t \sphinxbfcode{\sphinxupquote{blk\_execute\_rq}}}{struct request\sphinxstyleemphasis{ *rq}, bool\sphinxstyleemphasis{ at\_head}}{}
insert a request into queue for execution

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
request to insert

\item[{\sphinxcode{\sphinxupquote{bool at\_head}}}] \leavevmode
insert request at head or tail of queue

\end{description}

\sphinxstylestrong{Description}
\begin{quote}

Insert a fully prepared request at the back of the I/O scheduler queue
for execution and wait for completion.
\end{quote}

\sphinxstylestrong{Return}

The blk\_status\_t result provided to blk\_mq\_end\_request().
\index{blk\_mq\_delay\_run\_hw\_queue (C function)@\spxentry{blk\_mq\_delay\_run\_hw\_queue}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_delay_run_hw_queue}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_delay\_run\_hw\_queue}}}{struct {\hyperref[\detokenize{blk-mq:c.blk_mq_hw_ctx}]{\sphinxcrossref{blk\_mq\_hw\_ctx}}}\sphinxstyleemphasis{ *hctx}, unsigned long\sphinxstyleemphasis{ msecs}}{}
Run a hardware queue asynchronously.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct blk\_mq\_hw\_ctx *hctx}}}] \leavevmode
Pointer to the hardware queue to run.

\item[{\sphinxcode{\sphinxupquote{unsigned long msecs}}}] \leavevmode
Milliseconds of delay to wait before running the queue.

\end{description}

\sphinxstylestrong{Description}

Run a hardware queue asynchronously with a delay of \sphinxstylestrong{msecs}.
\index{blk\_mq\_run\_hw\_queue (C function)@\spxentry{blk\_mq\_run\_hw\_queue}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_run_hw_queue}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_run\_hw\_queue}}}{struct {\hyperref[\detokenize{blk-mq:c.blk_mq_hw_ctx}]{\sphinxcrossref{blk\_mq\_hw\_ctx}}}\sphinxstyleemphasis{ *hctx}, bool\sphinxstyleemphasis{ async}}{}
Start to run a hardware queue.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct blk\_mq\_hw\_ctx *hctx}}}] \leavevmode
Pointer to the hardware queue to run.

\item[{\sphinxcode{\sphinxupquote{bool async}}}] \leavevmode
If we want to run the queue asynchronously.

\end{description}

\sphinxstylestrong{Description}

Check if the request queue is not in a quiesced state and if there are
pending requests to be sent. If this is true, run the queue to send requests
to hardware.
\index{blk\_mq\_run\_hw\_queues (C function)@\spxentry{blk\_mq\_run\_hw\_queues}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_run_hw_queues}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_run\_hw\_queues}}}{struct request\_queue\sphinxstyleemphasis{ *q}, bool\sphinxstyleemphasis{ async}}{}
Run all hardware queues in a request queue.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request\_queue *q}}}] \leavevmode
Pointer to the request queue to run.

\item[{\sphinxcode{\sphinxupquote{bool async}}}] \leavevmode
If we want to run the queue asynchronously.

\end{description}
\index{blk\_mq\_delay\_run\_hw\_queues (C function)@\spxentry{blk\_mq\_delay\_run\_hw\_queues}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_delay_run_hw_queues}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_delay\_run\_hw\_queues}}}{struct request\_queue\sphinxstyleemphasis{ *q}, unsigned long\sphinxstyleemphasis{ msecs}}{}
Run all hardware queues asynchronously.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request\_queue *q}}}] \leavevmode
Pointer to the request queue to run.

\item[{\sphinxcode{\sphinxupquote{unsigned long msecs}}}] \leavevmode
Milliseconds of delay to wait before running the queues.

\end{description}
\index{blk\_mq\_request\_bypass\_insert (C function)@\spxentry{blk\_mq\_request\_bypass\_insert}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_request_bypass_insert}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_request\_bypass\_insert}}}{struct request\sphinxstyleemphasis{ *rq}, blk\_insert\_t\sphinxstyleemphasis{ flags}}{}
Insert a request at dispatch list.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
Pointer to request to be inserted.

\item[{\sphinxcode{\sphinxupquote{blk\_insert\_t flags}}}] \leavevmode
BLK\_MQ\_INSERT\_*

\end{description}

\sphinxstylestrong{Description}

Should only be used carefully, when the caller knows we want to
bypass a potential IO scheduler on the target device.
\index{blk\_mq\_try\_issue\_directly (C function)@\spxentry{blk\_mq\_try\_issue\_directly}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_try_issue_directly}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_try\_issue\_directly}}}{struct {\hyperref[\detokenize{blk-mq:c.blk_mq_hw_ctx}]{\sphinxcrossref{blk\_mq\_hw\_ctx}}}\sphinxstyleemphasis{ *hctx}, struct request\sphinxstyleemphasis{ *rq}}{}
Try to send a request directly to device driver.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct blk\_mq\_hw\_ctx *hctx}}}] \leavevmode
Pointer of the associated hardware queue.

\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
Pointer to request to be sent.

\end{description}

\sphinxstylestrong{Description}

If the device has enough resources to accept a new request now, send the
request directly to device driver. Else, insert at hctx\sphinxhyphen{}\textgreater{}dispatch queue, so
we can try send it another time in the future. Requests inserted at this
queue have higher priority.
\index{blk\_mq\_submit\_bio (C function)@\spxentry{blk\_mq\_submit\_bio}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_submit_bio}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_submit\_bio}}}{struct bio\sphinxstyleemphasis{ *bio}}{}
Create and send a request to block device.

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct bio *bio}}}] \leavevmode
Bio pointer.

\end{description}

\sphinxstylestrong{Description}

Builds up a request structure from \sphinxstylestrong{q} and \sphinxstylestrong{bio} and send to the device. The
request may not be queued directly to hardware if:
* This request can be merged with another one
* We want to place request at plug queue for possible future merging
* There is an IO scheduler active at this queue

It will not queue the request if there is an error with the bio, or at the
request creation.
\index{blk\_insert\_cloned\_request (C function)@\spxentry{blk\_insert\_cloned\_request}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_insert_cloned_request}}\pysiglinewithargsret{blk\_status\_t \sphinxbfcode{\sphinxupquote{blk\_insert\_cloned\_request}}}{struct request\sphinxstyleemphasis{ *rq}}{}
Helper for stacking drivers to submit a request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
the request being queued

\end{description}
\index{blk\_rq\_unprep\_clone (C function)@\spxentry{blk\_rq\_unprep\_clone}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_rq_unprep_clone}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_rq\_unprep\_clone}}}{struct request\sphinxstyleemphasis{ *rq}}{}
Helper function to free all bios in a cloned request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
the clone request to be cleaned up

\end{description}

\sphinxstylestrong{Description}
\begin{quote}

Free all bios in \sphinxstylestrong{rq} for a cloned request.
\end{quote}
\index{blk\_rq\_prep\_clone (C function)@\spxentry{blk\_rq\_prep\_clone}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_rq_prep_clone}}\pysiglinewithargsret{int \sphinxbfcode{\sphinxupquote{blk\_rq\_prep\_clone}}}{struct request\sphinxstyleemphasis{ *rq}, struct request\sphinxstyleemphasis{ *rq\_src}, struct bio\_set\sphinxstyleemphasis{ *bs}, gfp\_t\sphinxstyleemphasis{ gfp\_mask}, int (\sphinxstyleemphasis{*bio\_ctr})(struct bio *, struct bio *, void *), void\sphinxstyleemphasis{ *data}}{}
Helper function to setup clone request

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request *rq}}}] \leavevmode
the request to be setup

\item[{\sphinxcode{\sphinxupquote{struct request *rq\_src}}}] \leavevmode
original request to be cloned

\item[{\sphinxcode{\sphinxupquote{struct bio\_set *bs}}}] \leavevmode
bio\_set that bios for clone are allocated from

\item[{\sphinxcode{\sphinxupquote{gfp\_t gfp\_mask}}}] \leavevmode
memory allocation mask for bio

\item[{\sphinxcode{\sphinxupquote{int (*bio\_ctr)(struct bio *, struct bio *, void *)}}}] \leavevmode
setup function to be called for each clone bio.
Returns \sphinxcode{\sphinxupquote{0}} for success, non \sphinxcode{\sphinxupquote{0}} for failure.

\item[{\sphinxcode{\sphinxupquote{void *data}}}] \leavevmode
private data to be passed to \sphinxstylestrong{bio\_ctr}

\end{description}

\sphinxstylestrong{Description}
\begin{quote}

Clones bios in \sphinxstylestrong{rq\_src} to \sphinxstylestrong{rq}, and copies attributes of \sphinxstylestrong{rq\_src} to \sphinxstylestrong{rq}.
Also, pages which the original bios are pointing to are not copied
and the cloned bios just point same pages.
So cloned bios must be completed before original bios, which means
the caller must complete \sphinxstylestrong{rq} before \sphinxstylestrong{rq\_src}.
\end{quote}
\index{blk\_mq\_destroy\_queue (C function)@\spxentry{blk\_mq\_destroy\_queue}\spxextra{C function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{blk-mq:c.blk_mq_destroy_queue}}\pysiglinewithargsret{void \sphinxbfcode{\sphinxupquote{blk\_mq\_destroy\_queue}}}{struct request\_queue\sphinxstyleemphasis{ *q}}{}
shutdown a request queue

\end{fulllineitems}


\sphinxstylestrong{Parameters}
\begin{description}
\item[{\sphinxcode{\sphinxupquote{struct request\_queue *q}}}] \leavevmode
request queue to shutdown

\end{description}

\sphinxstylestrong{Description}

This shuts down a request queue allocated by blk\_mq\_init\_queue(). All future
requests will be failed with \sphinxhyphen{}ENODEV. The caller is responsible for dropping
the reference from blk\_mq\_init\_queue() by calling blk\_put\_queue().

\sphinxstylestrong{Context}

can sleep


\chapter{Embedded device command line partition parsing}
\label{\detokenize{cmdline-partition:embedded-device-command-line-partition-parsing}}\label{\detokenize{cmdline-partition::doc}}
The "blkdevparts" command line option adds support for reading the
block device partition table from the kernel command line.

It is typically used for fixed block (eMMC) embedded devices.
It has no MBR, so saves storage space. Bootloader can be easily accessed
by absolute address of data on the block device.
Users can easily change the partition.

The format for the command line is just like mtdparts:
\begin{description}
\item[{blkdevparts=\textless{}blkdev\sphinxhyphen{}def\textgreater{}{[};\textless{}blkdev\sphinxhyphen{}def\textgreater{}{]}}] \leavevmode\begin{description}
\item[{\textless{}blkdev\sphinxhyphen{}def\textgreater{} := \textless{}blkdev\sphinxhyphen{}id\textgreater{}:\textless{}partdef\textgreater{}{[},\textless{}partdef\textgreater{}{]}}] \leavevmode
\textless{}partdef\textgreater{} := \textless{}size\textgreater{}{[}@\textless{}offset\textgreater{}{]}(part\sphinxhyphen{}name)

\end{description}

\item[{\textless{}blkdev\sphinxhyphen{}id\textgreater{}}] \leavevmode
block device disk name. Embedded device uses fixed block device.
Its disk name is also fixed, such as: mmcblk0, mmcblk1, mmcblk0boot0.

\item[{\textless{}size\textgreater{}}] \leavevmode
partition size, in bytes, such as: 512, 1m, 1G.
size may contain an optional suffix of (upper or lower case):
\begin{quote}

K, M, G, T, P, E.
\end{quote}

"\sphinxhyphen{}" is used to denote all remaining space.

\item[{\textless{}offset\textgreater{}}] \leavevmode
partition start address, in bytes.
offset may contain an optional suffix of (upper or lower case):
\begin{quote}

K, M, G, T, P, E.
\end{quote}

\item[{(part\sphinxhyphen{}name)}] \leavevmode
partition name. Kernel sends uevent with "PARTNAME". Application can
create a link to block device partition with the name "PARTNAME".
User space application can access partition by partition name.

\end{description}

Example:
\begin{quote}
\begin{quote}

eMMC disk names are "mmcblk0" and "mmcblk0boot0".
\end{quote}

bootargs:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}blkdevparts=mmcblk0:1G(data0),1G(data1),\PYGZhy{};mmcblk0boot0:1m(boot),\PYGZhy{}(kernel)\PYGZsq{}
\end{sphinxVerbatim}

dmesg:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mmcblk0: p1(data0) p2(data1) p3()
mmcblk0boot0: p1(boot) p2(kernel)
\end{sphinxVerbatim}
\end{quote}


\chapter{Data Integrity}
\label{\detokenize{data-integrity:data-integrity}}\label{\detokenize{data-integrity::doc}}

\section{1. Introduction}
\label{\detokenize{data-integrity:introduction}}
Modern filesystems feature checksumming of data and metadata to
protect against data corruption.  However, the detection of the
corruption is done at read time which could potentially be months
after the data was written.  At that point the original data that the
application tried to write is most likely lost.

The solution is to ensure that the disk is actually storing what the
application meant it to.  Recent additions to both the SCSI family
protocols (SBC Data Integrity Field, SCC protection proposal) as well
as SATA/T13 (External Path Protection) try to remedy this by adding
support for appending integrity metadata to an I/O.  The integrity
metadata (or protection information in SCSI terminology) includes a
checksum for each sector as well as an incrementing counter that
ensures the individual sectors are written in the right order.  And
for some protection schemes also that the I/O is written to the right
place on disk.

Current storage controllers and devices implement various protective
measures, for instance checksumming and scrubbing.  But these
technologies are working in their own isolated domains or at best
between adjacent nodes in the I/O path.  The interesting thing about
DIF and the other integrity extensions is that the protection format
is well defined and every node in the I/O path can verify the
integrity of the I/O and reject it if corruption is detected.  This
allows not only corruption prevention but also isolation of the point
of failure.


\section{2. The Data Integrity Extensions}
\label{\detokenize{data-integrity:the-data-integrity-extensions}}
As written, the protocol extensions only protect the path between
controller and storage device.  However, many controllers actually
allow the operating system to interact with the integrity metadata
(IMD).  We have been working with several FC/SAS HBA vendors to enable
the protection information to be transferred to and from their
controllers.

The SCSI Data Integrity Field works by appending 8 bytes of protection
information to each sector.  The data + integrity metadata is stored
in 520 byte sectors on disk.  Data + IMD are interleaved when
transferred between the controller and target.  The T13 proposal is
similar.

Because it is highly inconvenient for operating systems to deal with
520 (and 4104) byte sectors, we approached several HBA vendors and
encouraged them to allow separation of the data and integrity metadata
scatter\sphinxhyphen{}gather lists.

The controller will interleave the buffers on write and split them on
read.  This means that Linux can DMA the data buffers to and from
host memory without changes to the page cache.

Also, the 16\sphinxhyphen{}bit CRC checksum mandated by both the SCSI and SATA specs
is somewhat heavy to compute in software.  Benchmarks found that
calculating this checksum had a significant impact on system
performance for a number of workloads.  Some controllers allow a
lighter\sphinxhyphen{}weight checksum to be used when interfacing with the operating
system.  Emulex, for instance, supports the TCP/IP checksum instead.
The IP checksum received from the OS is converted to the 16\sphinxhyphen{}bit CRC
when writing and vice versa.  This allows the integrity metadata to be
generated by Linux or the application at very low cost (comparable to
software RAID5).

The IP checksum is weaker than the CRC in terms of detecting bit
errors.  However, the strength is really in the separation of the data
buffers and the integrity metadata.  These two distinct buffers must
match up for an I/O to complete.

The separation of the data and integrity metadata buffers as well as
the choice in checksums is referred to as the Data Integrity
Extensions.  As these extensions are outside the scope of the protocol
bodies (T10, T13), Oracle and its partners are trying to standardize
them within the Storage Networking Industry Association.


\section{3. Kernel Changes}
\label{\detokenize{data-integrity:kernel-changes}}
The data integrity framework in Linux enables protection information
to be pinned to I/Os and sent to/received from controllers that
support it.

The advantage to the integrity extensions in SCSI and SATA is that
they enable us to protect the entire path from application to storage
device.  However, at the same time this is also the biggest
disadvantage. It means that the protection information must be in a
format that can be understood by the disk.

Generally Linux/POSIX applications are agnostic to the intricacies of
the storage devices they are accessing.  The virtual filesystem switch
and the block layer make things like hardware sector size and
transport protocols completely transparent to the application.

However, this level of detail is required when preparing the
protection information to send to a disk.  Consequently, the very
concept of an end\sphinxhyphen{}to\sphinxhyphen{}end protection scheme is a layering violation.
It is completely unreasonable for an application to be aware whether
it is accessing a SCSI or SATA disk.

The data integrity support implemented in Linux attempts to hide this
from the application.  As far as the application (and to some extent
the kernel) is concerned, the integrity metadata is opaque information
that\textquotesingle{}s attached to the I/O.

The current implementation allows the block layer to automatically
generate the protection information for any I/O.  Eventually the
intent is to move the integrity metadata calculation to userspace for
user data.  Metadata and other I/O that originates within the kernel
will still use the automatic generation interface.

Some storage devices allow each hardware sector to be tagged with a
16\sphinxhyphen{}bit value.  The owner of this tag space is the owner of the block
device.  I.e. the filesystem in most cases.  The filesystem can use
this extra space to tag sectors as they see fit.  Because the tag
space is limited, the block interface allows tagging bigger chunks by
way of interleaving.  This way, 8*16 bits of information can be
attached to a typical 4KB filesystem block.

This also means that applications such as fsck and mkfs will need
access to manipulate the tags from user space.  A passthrough
interface for this is being worked on.


\section{4. Block Layer Implementation Details}
\label{\detokenize{data-integrity:block-layer-implementation-details}}

\subsection{4.1 Bio}
\label{\detokenize{data-integrity:bio}}
The data integrity patches add a new field to struct bio when
CONFIG\_BLK\_DEV\_INTEGRITY is enabled.  bio\_integrity(bio) returns a
pointer to a struct bip which contains the bio integrity payload.
Essentially a bip is a trimmed down struct bio which holds a bio\_vec
containing the integrity metadata and the required housekeeping
information (bvec pool, vector count, etc.)

A kernel subsystem can enable data integrity protection on a bio by
calling bio\_integrity\_alloc(bio).  This will allocate and attach the
bip to the bio.

Individual pages containing integrity metadata can subsequently be
attached using bio\_integrity\_add\_page().

bio\_free() will automatically free the bip.


\subsection{4.2 Block Device}
\label{\detokenize{data-integrity:block-device}}
Because the format of the protection data is tied to the physical
disk, each block device has been extended with a block integrity
profile (struct blk\_integrity).  This optional profile is registered
with the block layer using blk\_integrity\_register().

The profile contains callback functions for generating and verifying
the protection data, as well as getting and setting application tags.
The profile also contains a few constants to aid in completing,
merging and splitting the integrity metadata.

Layered block devices will need to pick a profile that\textquotesingle{}s appropriate
for all subdevices.  blk\_integrity\_compare() can help with that.  DM
and MD linear, RAID0 and RAID1 are currently supported.  RAID4/5/6
will require extra work due to the application tag.


\section{5.0 Block Layer Integrity API}
\label{\detokenize{data-integrity:block-layer-integrity-api}}

\subsection{5.1 Normal Filesystem}
\label{\detokenize{data-integrity:normal-filesystem}}\begin{quote}

The normal filesystem is unaware that the underlying block device
is capable of sending/receiving integrity metadata.  The IMD will
be automatically generated by the block layer at submit\_bio() time
in case of a WRITE.  A READ request will cause the I/O integrity
to be verified upon completion.

IMD generation and verification can be toggled using the:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/sys/block/\PYGZlt{}bdev\PYGZgt{}/integrity/write\PYGZus{}generate
\end{sphinxVerbatim}

and:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/sys/block/\PYGZlt{}bdev\PYGZgt{}/integrity/read\PYGZus{}verify
\end{sphinxVerbatim}

flags.
\end{quote}


\subsection{5.2 Integrity\sphinxhyphen{}Aware Filesystem}
\label{\detokenize{data-integrity:integrity-aware-filesystem}}\begin{quote}

A filesystem that is integrity\sphinxhyphen{}aware can prepare I/Os with IMD
attached.  It can also use the application tag space if this is
supported by the block device.

\sphinxtitleref{bool bio\_integrity\_prep(bio);}
\begin{quote}

To generate IMD for WRITE and to set up buffers for READ, the
filesystem must call bio\_integrity\_prep(bio).

Prior to calling this function, the bio data direction and start
sector must be set, and the bio should have all data pages
added.  It is up to the caller to ensure that the bio does not
change while I/O is in progress.
Complete bio with error if prepare failed for some reson.
\end{quote}
\end{quote}


\subsection{5.3 Passing Existing Integrity Metadata}
\label{\detokenize{data-integrity:passing-existing-integrity-metadata}}\begin{quote}

Filesystems that either generate their own integrity metadata or
are capable of transferring IMD from user space can use the
following calls:

\sphinxtitleref{struct bip * bio\_integrity\_alloc(bio, gfp\_mask, nr\_pages);}
\begin{quote}

Allocates the bio integrity payload and hangs it off of the bio.
nr\_pages indicate how many pages of protection data need to be
stored in the integrity bio\_vec list (similar to bio\_alloc()).

The integrity payload will be freed at bio\_free() time.
\end{quote}

\sphinxtitleref{int bio\_integrity\_add\_page(bio, page, len, offset);}
\begin{quote}

Attaches a page containing integrity metadata to an existing
bio.  The bio must have an existing bip,
i.e. bio\_integrity\_alloc() must have been called.  For a WRITE,
the integrity metadata in the pages must be in a format
understood by the target device with the notable exception that
the sector numbers will be remapped as the request traverses the
I/O stack.  This implies that the pages added using this call
will be modified during I/O!  The first reference tag in the
integrity metadata must have a value of bip\sphinxhyphen{}\textgreater{}bip\_sector.

Pages can be added using bio\_integrity\_add\_page() as long as
there is room in the bip bio\_vec array (nr\_pages).

Upon completion of a READ operation, the attached pages will
contain the integrity metadata received from the storage device.
It is up to the receiver to process them and verify data
integrity upon completion.
\end{quote}
\end{quote}


\subsection{5.4 Registering A Block Device As Capable Of Exchanging Integrity Metadata}
\label{\detokenize{data-integrity:registering-a-block-device-as-capable-of-exchanging-integrity-metadata}}\begin{quote}

To enable integrity exchange on a block device the gendisk must be
registered as capable:

\sphinxtitleref{int blk\_integrity\_register(gendisk, blk\_integrity);}
\begin{quote}

The blk\_integrity struct is a template and should contain the
following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
static struct blk\PYGZus{}integrity my\PYGZus{}profile = \PYGZob{}
    .name                   = \PYGZdq{}STANDARDSBODY\PYGZhy{}TYPE\PYGZhy{}VARIANT\PYGZhy{}CSUM\PYGZdq{},
    .generate\PYGZus{}fn            = my\PYGZus{}generate\PYGZus{}fn,
    .verify\PYGZus{}fn              = my\PYGZus{}verify\PYGZus{}fn,
    .tuple\PYGZus{}size             = sizeof(struct my\PYGZus{}tuple\PYGZus{}size),
    .tag\PYGZus{}size               = \PYGZlt{}tag bytes per hw sector\PYGZgt{},
\PYGZcb{};
\end{sphinxVerbatim}

\textquotesingle{}name\textquotesingle{} is a text string which will be visible in sysfs.  This is
part of the userland API so chose it carefully and never change
it.  The format is standards body\sphinxhyphen{}type\sphinxhyphen{}variant.
E.g. T10\sphinxhyphen{}DIF\sphinxhyphen{}TYPE1\sphinxhyphen{}IP or T13\sphinxhyphen{}EPP\sphinxhyphen{}0\sphinxhyphen{}CRC.

\textquotesingle{}generate\_fn\textquotesingle{} generates appropriate integrity metadata (for WRITE).

\textquotesingle{}verify\_fn\textquotesingle{} verifies that the data buffer matches the integrity
metadata.

\textquotesingle{}tuple\_size\textquotesingle{} must be set to match the size of the integrity
metadata per sector.  I.e. 8 for DIF and EPP.

\textquotesingle{}tag\_size\textquotesingle{} must be set to identify how many bytes of tag space
are available per hardware sector.  For DIF this is either 2 or
0 depending on the value of the Control Mode Page ATO bit.
\end{quote}
\end{quote}


\bigskip\hrule\bigskip


2007\sphinxhyphen{}12\sphinxhyphen{}24 Martin K. Petersen \textless{}\sphinxhref{mailto:martin.petersen@oracle.com}{martin.petersen@oracle.com}\textgreater{}


\chapter{Deadline IO scheduler tunables}
\label{\detokenize{deadline-iosched:deadline-io-scheduler-tunables}}\label{\detokenize{deadline-iosched::doc}}
This little file attempts to document how the deadline io scheduler works.
In particular, it will clarify the meaning of the exposed tunables that may be
of interest to power users.


\section{Selecting IO schedulers}
\label{\detokenize{deadline-iosched:selecting-io-schedulers}}
Refer to Documentation/block/switching\sphinxhyphen{}sched.rst for information on
selecting an io scheduler on a per\sphinxhyphen{}device basis.


\bigskip\hrule\bigskip



\section{read\_expire     (in ms)}
\label{\detokenize{deadline-iosched:read-expire-in-ms}}
The goal of the deadline io scheduler is to attempt to guarantee a start
service time for a request. As we focus mainly on read latencies, this is
tunable. When a read request first enters the io scheduler, it is assigned
a deadline that is the current time + the read\_expire value in units of
milliseconds.


\section{write\_expire    (in ms)}
\label{\detokenize{deadline-iosched:write-expire-in-ms}}
Similar to read\_expire mentioned above, but for writes.


\section{fifo\_batch      (number of requests)}
\label{\detokenize{deadline-iosched:fifo-batch-number-of-requests}}
Requests are grouped into \sphinxcode{\sphinxupquote{batches}} of a particular data direction (read or
write) which are serviced in increasing sector order.  To limit extra seeking,
deadline expiries are only checked between batches.  fifo\_batch controls the
maximum number of requests per batch.

This parameter tunes the balance between per\sphinxhyphen{}request latency and aggregate
throughput.  When low latency is the primary concern, smaller is better (where
a value of 1 yields first\sphinxhyphen{}come first\sphinxhyphen{}served behaviour).  Increasing fifo\_batch
generally improves throughput, at the cost of latency variation.


\section{writes\_starved  (number of dispatches)}
\label{\detokenize{deadline-iosched:writes-starved-number-of-dispatches}}
When we have to move requests from the io scheduler queue to the block
device dispatch queue, we always give a preference to reads. However, we
don\textquotesingle{}t want to starve writes indefinitely either. So writes\_starved controls
how many times we give preference to reads over writes. When that has been
done writes\_starved number of times, we dispatch some writes based on the
same criteria as reads.


\section{front\_merges    (bool)}
\label{\detokenize{deadline-iosched:front-merges-bool}}
Sometimes it happens that a request enters the io scheduler that is contiguous
with a request that is already on the queue. Either it fits in the back of that
request, or it fits at the front. That is called either a back merge candidate
or a front merge candidate. Due to the way files are typically laid out,
back merges are much more common than front merges. For some work loads, you
may even know that it is a waste of time to spend any time attempting to
front merge requests. Setting front\_merges to 0 disables this functionality.
Front merges may still occur due to the cached last\_merge hint, but since
that comes at basically 0 cost we leave that on. We simply disable the
rbtree front sector lookup when the io scheduler merge function is called.

Nov 11 2002, Jens Axboe \textless{}\sphinxhref{mailto:jens.axboe@oracle.com}{jens.axboe@oracle.com}\textgreater{}


\chapter{Inline Encryption}
\label{\detokenize{inline-encryption:inline-encryption-1}}\label{\detokenize{inline-encryption:inline-encryption}}\label{\detokenize{inline-encryption::doc}}

\section{Background}
\label{\detokenize{inline-encryption:background}}
Inline encryption hardware sits logically between memory and disk, and can
en/decrypt data as it goes in/out of the disk.  For each I/O request, software
can control exactly how the inline encryption hardware will en/decrypt the data
in terms of key, algorithm, data unit size (the granularity of en/decryption),
and data unit number (a value that determines the initialization vector(s)).

Some inline encryption hardware accepts all encryption parameters including raw
keys directly in low\sphinxhyphen{}level I/O requests.  However, most inline encryption
hardware instead has a fixed number of "keyslots" and requires that the key,
algorithm, and data unit size first be programmed into a keyslot.  Each
low\sphinxhyphen{}level I/O request then just contains a keyslot index and data unit number.

Note that inline encryption hardware is very different from traditional crypto
accelerators, which are supported through the kernel crypto API.  Traditional
crypto accelerators operate on memory regions, whereas inline encryption
hardware operates on I/O requests.  Thus, inline encryption hardware needs to be
managed by the block layer, not the kernel crypto API.

Inline encryption hardware is also very different from "self\sphinxhyphen{}encrypting drives",
such as those based on the TCG Opal or ATA Security standards.  Self\sphinxhyphen{}encrypting
drives don\textquotesingle{}t provide fine\sphinxhyphen{}grained control of encryption and provide no way to
verify the correctness of the resulting ciphertext.  Inline encryption hardware
provides fine\sphinxhyphen{}grained control of encryption, including the choice of key and
initialization vector for each sector, and can be tested for correctness.


\section{Objective}
\label{\detokenize{inline-encryption:objective}}
We want to support inline encryption in the kernel.  To make testing easier, we
also want support for falling back to the kernel crypto API when actual inline
encryption hardware is absent.  We also want inline encryption to work with
layered devices like device\sphinxhyphen{}mapper and loopback (i.e. we want to be able to use
the inline encryption hardware of the underlying devices if present, or else
fall back to crypto API en/decryption).


\section{Constraints and notes}
\label{\detokenize{inline-encryption:constraints-and-notes}}\begin{itemize}
\item {} 
We need a way for upper layers (e.g. filesystems) to specify an encryption
context to use for en/decrypting a bio, and device drivers (e.g. UFSHCD) need
to be able to use that encryption context when they process the request.
Encryption contexts also introduce constraints on bio merging; the block layer
needs to be aware of these constraints.

\item {} 
Different inline encryption hardware has different supported algorithms,
supported data unit sizes, maximum data unit numbers, etc.  We call these
properties the "crypto capabilities".  We need a way for device drivers to
advertise crypto capabilities to upper layers in a generic way.

\item {} 
Inline encryption hardware usually (but not always) requires that keys be
programmed into keyslots before being used.  Since programming keyslots may be
slow and there may not be very many keyslots, we shouldn\textquotesingle{}t just program the
key for every I/O request, but rather keep track of which keys are in the
keyslots and reuse an already\sphinxhyphen{}programmed keyslot when possible.

\item {} 
Upper layers typically define a specific end\sphinxhyphen{}of\sphinxhyphen{}life for crypto keys, e.g.
when an encrypted directory is locked or when a crypto mapping is torn down.
At these times, keys are wiped from memory.  We must provide a way for upper
layers to also evict keys from any keyslots they are present in.

\item {} 
When possible, device\sphinxhyphen{}mapper devices must be able to pass through the inline
encryption support of their underlying devices.  However, it doesn\textquotesingle{}t make
sense for device\sphinxhyphen{}mapper devices to have keyslots themselves.

\end{itemize}


\section{Basic design}
\label{\detokenize{inline-encryption:basic-design}}
We introduce \sphinxcode{\sphinxupquote{struct blk\_crypto\_key}} to represent an inline encryption key and
how it will be used.  This includes the actual bytes of the key; the size of the
key; the algorithm and data unit size the key will be used with; and the number
of bytes needed to represent the maximum data unit number the key will be used
with.

We introduce \sphinxcode{\sphinxupquote{struct bio\_crypt\_ctx}} to represent an encryption context.  It
contains a data unit number and a pointer to a blk\_crypto\_key.  We add pointers
to a bio\_crypt\_ctx to \sphinxcode{\sphinxupquote{struct bio}} and \sphinxcode{\sphinxupquote{struct request}}; this allows users
of the block layer (e.g. filesystems) to provide an encryption context when
creating a bio and have it be passed down the stack for processing by the block
layer and device drivers.  Note that the encryption context doesn\textquotesingle{}t explicitly
say whether to encrypt or decrypt, as that is implicit from the direction of the
bio; WRITE means encrypt, and READ means decrypt.

We also introduce \sphinxcode{\sphinxupquote{struct blk\_crypto\_profile}} to contain all generic inline
encryption\sphinxhyphen{}related state for a particular inline encryption device.  The
blk\_crypto\_profile serves as the way that drivers for inline encryption hardware
advertise their crypto capabilities and provide certain functions (e.g.,
functions to program and evict keys) to upper layers.  Each device driver that
wants to support inline encryption will construct a blk\_crypto\_profile, then
associate it with the disk\textquotesingle{}s request\_queue.

The blk\_crypto\_profile also manages the hardware\textquotesingle{}s keyslots, when applicable.
This happens in the block layer, so that users of the block layer can just
specify encryption contexts and don\textquotesingle{}t need to know about keyslots at all, nor do
device drivers need to care about most details of keyslot management.

Specifically, for each keyslot, the block layer (via the blk\_crypto\_profile)
keeps track of which blk\_crypto\_key that keyslot contains (if any), and how many
in\sphinxhyphen{}flight I/O requests are using it.  When the block layer creates a
\sphinxcode{\sphinxupquote{struct request}} for a bio that has an encryption context, it grabs a keyslot
that already contains the key if possible.  Otherwise it waits for an idle
keyslot (a keyslot that isn\textquotesingle{}t in\sphinxhyphen{}use by any I/O), then programs the key into the
least\sphinxhyphen{}recently\sphinxhyphen{}used idle keyslot using the function the device driver provided.
In both cases, the resulting keyslot is stored in the \sphinxcode{\sphinxupquote{crypt\_keyslot}} field of
the request, where it is then accessible to device drivers and is released after
the request completes.

\sphinxcode{\sphinxupquote{struct request}} also contains a pointer to the original bio\_crypt\_ctx.
Requests can be built from multiple bios, and the block layer must take the
encryption context into account when trying to merge bios and requests.  For two
bios/requests to be merged, they must have compatible encryption contexts: both
unencrypted, or both encrypted with the same key and contiguous data unit
numbers.  Only the encryption context for the first bio in a request is
retained, since the remaining bios have been verified to be merge\sphinxhyphen{}compatible
with the first bio.

To make it possible for inline encryption to work with request\_queue based
layered devices, when a request is cloned, its encryption context is cloned as
well.  When the cloned request is submitted, it is then processed as usual; this
includes getting a keyslot from the clone\textquotesingle{}s target device if needed.


\section{blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback}
\label{\detokenize{inline-encryption:blk-crypto-fallback}}
It is desirable for the inline encryption support of upper layers (e.g.
filesystems) to be testable without real inline encryption hardware, and
likewise for the block layer\textquotesingle{}s keyslot management logic.  It is also desirable
to allow upper layers to just always use inline encryption rather than have to
implement encryption in multiple ways.

Therefore, we also introduce \sphinxstyleemphasis{blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback}, which is an implementation
of inline encryption using the kernel crypto API.  blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback is built
into the block layer, so it works on any block device without any special setup.
Essentially, when a bio with an encryption context is submitted to a
block\_device that doesn\textquotesingle{}t support that encryption context, the block layer will
handle en/decryption of the bio using blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback.

For encryption, the data cannot be encrypted in\sphinxhyphen{}place, as callers usually rely
on it being unmodified.  Instead, blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback allocates bounce pages,
fills a new bio with those bounce pages, encrypts the data into those bounce
pages, and submits that "bounce" bio.  When the bounce bio completes,
blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback completes the original bio.  If the original bio is too
large, multiple bounce bios may be required; see the code for details.

For decryption, blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback "wraps" the bio\textquotesingle{}s completion callback
(\sphinxcode{\sphinxupquote{bi\_complete}}) and private data (\sphinxcode{\sphinxupquote{bi\_private}}) with its own, unsets the
bio\textquotesingle{}s encryption context, then submits the bio.  If the read completes
successfully, blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback restores the bio\textquotesingle{}s original completion
callback and private data, then decrypts the bio\textquotesingle{}s data in\sphinxhyphen{}place using the
kernel crypto API.  Decryption happens from a workqueue, as it may sleep.
Afterwards, blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback completes the bio.

In both cases, the bios that blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback submits no longer have an
encryption context.  Therefore, lower layers only see standard unencrypted I/O.

blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback also defines its own blk\_crypto\_profile and has its own
"keyslots"; its keyslots contain \sphinxcode{\sphinxupquote{struct crypto\_skcipher}} objects.  The reason
for this is twofold.  First, it allows the keyslot management logic to be tested
without actual inline encryption hardware.  Second, similar to actual inline
encryption hardware, the crypto API doesn\textquotesingle{}t accept keys directly in requests but
rather requires that keys be set ahead of time, and setting keys can be
expensive; moreover, allocating a crypto\_skcipher can\textquotesingle{}t happen on the I/O path
at all due to the locks it takes.  Therefore, the concept of keyslots still
makes sense for blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback.

Note that regardless of whether real inline encryption hardware or
blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback is used, the ciphertext written to disk (and hence the
on\sphinxhyphen{}disk format of data) will be the same (assuming that both the inline
encryption hardware\textquotesingle{}s implementation and the kernel crypto API\textquotesingle{}s implementation
of the algorithm being used adhere to spec and function correctly).

blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback is optional and is controlled by the
\sphinxcode{\sphinxupquote{CONFIG\_BLK\_INLINE\_ENCRYPTION\_FALLBACK}} kernel configuration option.


\section{API presented to users of the block layer}
\label{\detokenize{inline-encryption:api-presented-to-users-of-the-block-layer}}
\sphinxcode{\sphinxupquote{blk\_crypto\_config\_supported()}} allows users to check ahead of time whether
inline encryption with particular crypto settings will work on a particular
block\_device \sphinxhyphen{}\sphinxhyphen{} either via hardware or via blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback.  This function
takes in a \sphinxcode{\sphinxupquote{struct blk\_crypto\_config}} which is like blk\_crypto\_key, but omits
the actual bytes of the key and instead just contains the algorithm, data unit
size, etc.  This function can be useful if blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback is disabled.

\sphinxcode{\sphinxupquote{blk\_crypto\_init\_key()}} allows users to initialize a blk\_crypto\_key.

Users must call \sphinxcode{\sphinxupquote{blk\_crypto\_start\_using\_key()}} before actually starting to use
a blk\_crypto\_key on a block\_device (even if \sphinxcode{\sphinxupquote{blk\_crypto\_config\_supported()}}
was called earlier).  This is needed to initialize blk\sphinxhyphen{}crypto\sphinxhyphen{}fallback if it
will be needed.  This must not be called from the data path, as this may have to
allocate resources, which may deadlock in that case.

Next, to attach an encryption context to a bio, users should call
\sphinxcode{\sphinxupquote{bio\_crypt\_set\_ctx()}}.  This function allocates a bio\_crypt\_ctx and attaches
it to a bio, given the blk\_crypto\_key and the data unit number that will be used
for en/decryption.  Users don\textquotesingle{}t need to worry about freeing the bio\_crypt\_ctx
later, as that happens automatically when the bio is freed or reset.

Finally, when done using inline encryption with a blk\_crypto\_key on a
block\_device, users must call \sphinxcode{\sphinxupquote{blk\_crypto\_evict\_key()}}.  This ensures that
the key is evicted from all keyslots it may be programmed into and unlinked from
any kernel data structures it may be linked into.

In summary, for users of the block layer, the lifecycle of a blk\_crypto\_key is
as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxcode{\sphinxupquote{blk\_crypto\_config\_supported()}} (optional)

\item {} 
\sphinxcode{\sphinxupquote{blk\_crypto\_init\_key()}}

\item {} 
\sphinxcode{\sphinxupquote{blk\_crypto\_start\_using\_key()}}

\item {} 
\sphinxcode{\sphinxupquote{bio\_crypt\_set\_ctx()}} (potentially many times)

\item {} 
\sphinxcode{\sphinxupquote{blk\_crypto\_evict\_key()}} (after all I/O has completed)

\item {} 
Zeroize the blk\_crypto\_key (this has no dedicated function)

\end{enumerate}

If a blk\_crypto\_key is being used on multiple block\_devices, then
\sphinxcode{\sphinxupquote{blk\_crypto\_config\_supported()}} (if used), \sphinxcode{\sphinxupquote{blk\_crypto\_start\_using\_key()}},
and \sphinxcode{\sphinxupquote{blk\_crypto\_evict\_key()}} must be called on each block\_device.


\section{API presented to device drivers}
\label{\detokenize{inline-encryption:api-presented-to-device-drivers}}
A device driver that wants to support inline encryption must set up a
blk\_crypto\_profile in the request\_queue of its device.  To do this, it first
must call \sphinxcode{\sphinxupquote{blk\_crypto\_profile\_init()}} (or its resource\sphinxhyphen{}managed variant
\sphinxcode{\sphinxupquote{devm\_blk\_crypto\_profile\_init()}}), providing the number of keyslots.

Next, it must advertise its crypto capabilities by setting fields in the
blk\_crypto\_profile, e.g. \sphinxcode{\sphinxupquote{modes\_supported}} and \sphinxcode{\sphinxupquote{max\_dun\_bytes\_supported}}.

It then must set function pointers in the \sphinxcode{\sphinxupquote{ll\_ops}} field of the
blk\_crypto\_profile to tell upper layers how to control the inline encryption
hardware, e.g. how to program and evict keyslots.  Most drivers will need to
implement \sphinxcode{\sphinxupquote{keyslot\_program}} and \sphinxcode{\sphinxupquote{keyslot\_evict}}.  For details, see the
comments for \sphinxcode{\sphinxupquote{struct blk\_crypto\_ll\_ops}}.

Once the driver registers a blk\_crypto\_profile with a request\_queue, I/O
requests the driver receives via that queue may have an encryption context.  All
encryption contexts will be compatible with the crypto capabilities declared in
the blk\_crypto\_profile, so drivers don\textquotesingle{}t need to worry about handling
unsupported requests.  Also, if a nonzero number of keyslots was declared in the
blk\_crypto\_profile, then all I/O requests that have an encryption context will
also have a keyslot which was already programmed with the appropriate key.

If the driver implements runtime suspend and its blk\_crypto\_ll\_ops don\textquotesingle{}t work
while the device is runtime\sphinxhyphen{}suspended, then the driver must also set the \sphinxcode{\sphinxupquote{dev}}
field of the blk\_crypto\_profile to point to the \sphinxcode{\sphinxupquote{struct device}} that will be
resumed before any of the low\sphinxhyphen{}level operations are called.

If there are situations where the inline encryption hardware loses the contents
of its keyslots, e.g. device resets, the driver must handle reprogramming the
keyslots.  To do this, the driver may call \sphinxcode{\sphinxupquote{blk\_crypto\_reprogram\_all\_keys()}}.

Finally, if the driver used \sphinxcode{\sphinxupquote{blk\_crypto\_profile\_init()}} instead of
\sphinxcode{\sphinxupquote{devm\_blk\_crypto\_profile\_init()}}, then it is responsible for calling
\sphinxcode{\sphinxupquote{blk\_crypto\_profile\_destroy()}} when the crypto profile is no longer needed.


\section{Layered Devices}
\label{\detokenize{inline-encryption:layered-devices}}
Request queue based layered devices like dm\sphinxhyphen{}rq that wish to support inline
encryption need to create their own blk\_crypto\_profile for their request\_queue,
and expose whatever functionality they choose. When a layered device wants to
pass a clone of that request to another request\_queue, blk\sphinxhyphen{}crypto will
initialize and prepare the clone as necessary.


\section{Interaction between inline encryption and blk integrity}
\label{\detokenize{inline-encryption:interaction-between-inline-encryption-and-blk-integrity}}
At the time of this patch, there is no real hardware that supports both these
features. However, these features do interact with each other, and it\textquotesingle{}s not
completely trivial to make them both work together properly. In particular,
when a WRITE bio wants to use inline encryption on a device that supports both
features, the bio will have an encryption context specified, after which
its integrity information is calculated (using the plaintext data, since
the encryption will happen while data is being written), and the data and
integrity info is sent to the device. Obviously, the integrity info must be
verified before the data is encrypted. After the data is encrypted, the device
must not store the integrity info that it received with the plaintext data
since that might reveal information about the plaintext data. As such, it must
re\sphinxhyphen{}generate the integrity info from the ciphertext data and store that on disk
instead. Another issue with storing the integrity info of the plaintext data is
that it changes the on disk format depending on whether hardware inline
encryption support is present or the kernel crypto API fallback is used (since
if the fallback is used, the device will receive the integrity info of the
ciphertext, not that of the plaintext).

Because there isn\textquotesingle{}t any real hardware yet, it seems prudent to assume that
hardware implementations might not implement both features together correctly,
and disallow the combination for now. Whenever a device supports integrity, the
kernel will pretend that the device does not support hardware inline encryption
(by setting the blk\_crypto\_profile in the request\_queue of the device to NULL).
When the crypto API fallback is enabled, this means that all bios with and
encryption context will use the fallback, and IO will complete as usual.  When
the fallback is disabled, a bio with an encryption context will be failed.


\chapter{Block io priorities}
\label{\detokenize{ioprio:block-io-priorities}}\label{\detokenize{ioprio::doc}}

\section{Intro}
\label{\detokenize{ioprio:intro}}
With the introduction of cfq v3 (aka cfq\sphinxhyphen{}ts or time sliced cfq), basic io
priorities are supported for reads on files.  This enables users to io nice
processes or process groups, similar to what has been possible with cpu
scheduling for ages.  This document mainly details the current possibilities
with cfq; other io schedulers do not support io priorities thus far.


\section{Scheduling classes}
\label{\detokenize{ioprio:scheduling-classes}}
CFQ implements three generic scheduling classes that determine how io is
served for a process.

IOPRIO\_CLASS\_RT: This is the realtime io class. This scheduling class is given
higher priority than any other in the system, processes from this class are
given first access to the disk every time. Thus it needs to be used with some
care, one io RT process can starve the entire system. Within the RT class,
there are 8 levels of class data that determine exactly how much time this
process needs the disk for on each service. In the future this might change
to be more directly mappable to performance, by passing in a wanted data
rate instead.

IOPRIO\_CLASS\_BE: This is the best\sphinxhyphen{}effort scheduling class, which is the default
for any process that hasn\textquotesingle{}t set a specific io priority. The class data
determines how much io bandwidth the process will get, it\textquotesingle{}s directly mappable
to the cpu nice levels just more coarsely implemented. 0 is the highest
BE prio level, 7 is the lowest. The mapping between cpu nice level and io
nice level is determined as: io\_nice = (cpu\_nice + 20) / 5.

IOPRIO\_CLASS\_IDLE: This is the idle scheduling class, processes running at this
level only get io time when no one else needs the disk. The idle class has no
class data, since it doesn\textquotesingle{}t really apply here.


\section{Tools}
\label{\detokenize{ioprio:tools}}
See below for a sample ionice tool. Usage:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} ionice \PYGZhy{}c\PYGZlt{}class\PYGZgt{} \PYGZhy{}n\PYGZlt{}level\PYGZgt{} \PYGZhy{}p\PYGZlt{}pid\PYGZgt{}
\end{sphinxVerbatim}

If pid isn\textquotesingle{}t given, the current process is assumed. IO priority settings
are inherited on fork, so you can use ionice to start the process at a given
level:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} ionice \PYGZhy{}c2 \PYGZhy{}n0 /bin/ls
\end{sphinxVerbatim}

will run ls at the best\sphinxhyphen{}effort scheduling class at the highest priority.
For a running process, you can give the pid instead:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} ionice \PYGZhy{}c1 \PYGZhy{}n2 \PYGZhy{}p100
\end{sphinxVerbatim}

will change pid 100 to run at the realtime scheduling class, at priority 2.

ionice.c tool:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{}include \PYGZlt{}stdio.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}stdlib.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}errno.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}getopt.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}unistd.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}sys/ptrace.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}asm/unistd.h\PYGZgt{}

extern int sys\PYGZus{}ioprio\PYGZus{}set(int, int, int);
extern int sys\PYGZus{}ioprio\PYGZus{}get(int, int);

\PYGZsh{}if defined(\PYGZus{}\PYGZus{}i386\PYGZus{}\PYGZus{})
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}set               289
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}get               290
\PYGZsh{}elif defined(\PYGZus{}\PYGZus{}ppc\PYGZus{}\PYGZus{})
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}set               273
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}get               274
\PYGZsh{}elif defined(\PYGZus{}\PYGZus{}x86\PYGZus{}64\PYGZus{}\PYGZus{})
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}set               251
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}get               252
\PYGZsh{}elif defined(\PYGZus{}\PYGZus{}ia64\PYGZus{}\PYGZus{})
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}set               1274
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}get               1275
\PYGZsh{}else
\PYGZsh{}error \PYGZdq{}Unsupported arch\PYGZdq{}
\PYGZsh{}endif

static inline int ioprio\PYGZus{}set(int which, int who, int ioprio)
\PYGZob{}
      return syscall(\PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}set, which, who, ioprio);
\PYGZcb{}

static inline int ioprio\PYGZus{}get(int which, int who)
\PYGZob{}
      return syscall(\PYGZus{}\PYGZus{}NR\PYGZus{}ioprio\PYGZus{}get, which, who);
\PYGZcb{}

enum \PYGZob{}
      IOPRIO\PYGZus{}CLASS\PYGZus{}NONE,
      IOPRIO\PYGZus{}CLASS\PYGZus{}RT,
      IOPRIO\PYGZus{}CLASS\PYGZus{}BE,
      IOPRIO\PYGZus{}CLASS\PYGZus{}IDLE,
\PYGZcb{};

enum \PYGZob{}
      IOPRIO\PYGZus{}WHO\PYGZus{}PROCESS = 1,
      IOPRIO\PYGZus{}WHO\PYGZus{}PGRP,
      IOPRIO\PYGZus{}WHO\PYGZus{}USER,
\PYGZcb{};

\PYGZsh{}define IOPRIO\PYGZus{}CLASS\PYGZus{}SHIFT    13

const char *to\PYGZus{}prio[] = \PYGZob{} \PYGZdq{}none\PYGZdq{}, \PYGZdq{}realtime\PYGZdq{}, \PYGZdq{}best\PYGZhy{}effort\PYGZdq{}, \PYGZdq{}idle\PYGZdq{}, \PYGZcb{};

int main(int argc, char *argv[])
\PYGZob{}
      int ioprio = 4, set = 0, ioprio\PYGZus{}class = IOPRIO\PYGZus{}CLASS\PYGZus{}BE;
      int c, pid = 0;

      while ((c = getopt(argc, argv, \PYGZdq{}+n:c:p:\PYGZdq{})) != EOF) \PYGZob{}
              switch (c) \PYGZob{}
              case \PYGZsq{}n\PYGZsq{}:
                      ioprio = strtol(optarg, NULL, 10);
                      set = 1;
                      break;
              case \PYGZsq{}c\PYGZsq{}:
                      ioprio\PYGZus{}class = strtol(optarg, NULL, 10);
                      set = 1;
                      break;
              case \PYGZsq{}p\PYGZsq{}:
                      pid = strtol(optarg, NULL, 10);
                      break;
              \PYGZcb{}
      \PYGZcb{}

      switch (ioprio\PYGZus{}class) \PYGZob{}
              case IOPRIO\PYGZus{}CLASS\PYGZus{}NONE:
                      ioprio\PYGZus{}class = IOPRIO\PYGZus{}CLASS\PYGZus{}BE;
                      break;
              case IOPRIO\PYGZus{}CLASS\PYGZus{}RT:
              case IOPRIO\PYGZus{}CLASS\PYGZus{}BE:
                      break;
              case IOPRIO\PYGZus{}CLASS\PYGZus{}IDLE:
                      ioprio = 7;
                      break;
              default:
                      printf(\PYGZdq{}bad prio class \PYGZpc{}d\PYGZbs{}n\PYGZdq{}, ioprio\PYGZus{}class);
                      return 1;
      \PYGZcb{}

      if (!set) \PYGZob{}
              if (!pid \PYGZam{}\PYGZam{} argv[optind])
                      pid = strtol(argv[optind], NULL, 10);

              ioprio = ioprio\PYGZus{}get(IOPRIO\PYGZus{}WHO\PYGZus{}PROCESS, pid);

              printf(\PYGZdq{}pid=\PYGZpc{}d, \PYGZpc{}d\PYGZbs{}n\PYGZdq{}, pid, ioprio);

              if (ioprio == \PYGZhy{}1)
                      perror(\PYGZdq{}ioprio\PYGZus{}get\PYGZdq{});
              else \PYGZob{}
                      ioprio\PYGZus{}class = ioprio \PYGZgt{}\PYGZgt{} IOPRIO\PYGZus{}CLASS\PYGZus{}SHIFT;
                      ioprio = ioprio \PYGZam{} 0xff;
                      printf(\PYGZdq{}\PYGZpc{}s: prio \PYGZpc{}d\PYGZbs{}n\PYGZdq{}, to\PYGZus{}prio[ioprio\PYGZus{}class], ioprio);
              \PYGZcb{}
      \PYGZcb{} else \PYGZob{}
              if (ioprio\PYGZus{}set(IOPRIO\PYGZus{}WHO\PYGZus{}PROCESS, pid, ioprio | ioprio\PYGZus{}class \PYGZlt{}\PYGZlt{} IOPRIO\PYGZus{}CLASS\PYGZus{}SHIFT) == \PYGZhy{}1) \PYGZob{}
                      perror(\PYGZdq{}ioprio\PYGZus{}set\PYGZdq{});
                      return 1;
              \PYGZcb{}

              if (argv[optind])
                      execvp(argv[optind], \PYGZam{}argv[optind]);
      \PYGZcb{}

      return 0;
\PYGZcb{}
\end{sphinxVerbatim}

March 11 2005, Jens Axboe \textless{}\sphinxhref{mailto:jens.axboe@oracle.com}{jens.axboe@oracle.com}\textgreater{}


\chapter{Kyber I/O scheduler tunables}
\label{\detokenize{kyber-iosched:kyber-i-o-scheduler-tunables}}\label{\detokenize{kyber-iosched::doc}}
The only two tunables for the Kyber scheduler are the target latencies for
reads and synchronous writes. Kyber will throttle requests in order to meet
these target latencies.


\section{read\_lat\_nsec}
\label{\detokenize{kyber-iosched:read-lat-nsec}}
Target latency for reads (in nanoseconds).


\section{write\_lat\_nsec}
\label{\detokenize{kyber-iosched:write-lat-nsec}}
Target latency for synchronous writes (in nanoseconds).


\chapter{Null block device driver}
\label{\detokenize{null_blk:null-block-device-driver}}\label{\detokenize{null_blk::doc}}

\section{Overview}
\label{\detokenize{null_blk:overview}}
The null block device (\sphinxcode{\sphinxupquote{/dev/nullb*}}) is used for benchmarking the various
block\sphinxhyphen{}layer implementations. It emulates a block device of X gigabytes in size.
It does not execute any read/write operation, just mark them as complete in
the request queue. The following instances are possible:
\begin{quote}

Multi\sphinxhyphen{}queue block\sphinxhyphen{}layer
\begin{itemize}
\item {} 
Request\sphinxhyphen{}based.

\item {} 
Configurable submission queues per device.

\end{itemize}

No block\sphinxhyphen{}layer (Known as bio\sphinxhyphen{}based)
\begin{itemize}
\item {} 
Bio\sphinxhyphen{}based. IO requests are submitted directly to the device driver.

\item {} 
Directly accepts bio data structure and returns them.

\end{itemize}
\end{quote}

All of them have a completion queue for each core in the system.


\section{Module parameters}
\label{\detokenize{null_blk:module-parameters}}\begin{description}
\item[{queue\_mode={[}0\sphinxhyphen{}2{]}: Default: 2\sphinxhyphen{}Multi\sphinxhyphen{}queue}] \leavevmode
Selects which block\sphinxhyphen{}layer the module should instantiate with.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
Bio\sphinxhyphen{}based
\\
\hline
1
&
Single\sphinxhyphen{}queue (deprecated)
\\
\hline
2
&
Multi\sphinxhyphen{}queue
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{home\_node={[}0\sphinxhyphen{}\sphinxhyphen{}nr\_nodes{]}: Default: NUMA\_NO\_NODE}] \leavevmode
Selects what CPU node the data structures are allocated from.

\item[{gb={[}Size in GB{]}: Default: 250GB}] \leavevmode
The size of the device reported to the system.

\item[{bs={[}Block size (in bytes){]}: Default: 512 bytes}] \leavevmode
The block size reported to the system.

\item[{nr\_devices={[}Number of devices{]}: Default: 1}] \leavevmode
Number of block devices instantiated. They are instantiated as /dev/nullb0,
etc.

\item[{irqmode={[}0\sphinxhyphen{}2{]}: Default: 1\sphinxhyphen{}Soft\sphinxhyphen{}irq}] \leavevmode
The completion mode used for completing IOs to the block\sphinxhyphen{}layer.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
None.
\\
\hline
1
&
Soft\sphinxhyphen{}irq. Uses IPI to complete IOs across CPU nodes. Simulates the overhead
when IOs are issued from another CPU node than the home the device is
connected to.
\\
\hline
2
&
Timer: Waits a specific period (completion\_nsec) for each IO before
completion.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{completion\_nsec={[}ns{]}: Default: 10,000ns}] \leavevmode
Combined with irqmode=2 (timer). The time each completion event must wait.

\item[{submit\_queues={[}1..nr\_cpus{]}: Default: 1}] \leavevmode
The number of submission queues attached to the device driver. If unset, it
defaults to 1. For multi\sphinxhyphen{}queue, it is ignored when use\_per\_node\_hctx module
parameter is 1.

\item[{hw\_queue\_depth={[}0..qdepth{]}: Default: 64}] \leavevmode
The hardware queue depth of the device.

\item[{memory\_backed={[}0/1{]}: Default: 0}] \leavevmode
Whether or not to use a memory buffer to respond to IO requests


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
Transfer no data in response to IO requests
\\
\hline
1
&
Use a memory buffer to respond to IO requests
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{discard={[}0/1{]}: Default: 0}] \leavevmode
Support discard operations (requires memory\sphinxhyphen{}backed null\_blk device).


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
Do not support discard operations
\\
\hline
1
&
Enable support for discard operations
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{cache\_size={[}Size in MB{]}: Default: 0}] \leavevmode
Cache size in MB for memory\sphinxhyphen{}backed device.

\item[{mbps={[}Maximum bandwidth in MB/s{]}: Default: 0 (no limit)}] \leavevmode
Bandwidth limit for device performance.

\end{description}


\subsection{Multi\sphinxhyphen{}queue specific parameters}
\label{\detokenize{null_blk:multi-queue-specific-parameters}}\begin{description}
\item[{use\_per\_node\_hctx={[}0/1{]}: Default: 0}] \leavevmode
Number of hardware context queues.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
The number of submit queues are set to the value of the submit\_queues
parameter.
\\
\hline
1
&
The multi\sphinxhyphen{}queue block layer is instantiated with a hardware dispatch
queue for each CPU node in the system.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{no\_sched={[}0/1{]}: Default: 0}] \leavevmode
Enable/disable the io scheduler.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
nullb* use default blk\sphinxhyphen{}mq io scheduler
\\
\hline
1
&
nullb* doesn\textquotesingle{}t use io scheduler
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{blocking={[}0/1{]}: Default: 0}] \leavevmode
Blocking behavior of the request queue.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
Register as a non\sphinxhyphen{}blocking blk\sphinxhyphen{}mq driver device.
\\
\hline
1
&
Register as a blocking blk\sphinxhyphen{}mq driver device, null\_blk will set
the BLK\_MQ\_F\_BLOCKING flag, indicating that it sometimes/always
needs to block in its \sphinxhyphen{}\textgreater{}queue\_rq() function.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{shared\_tags={[}0/1{]}: Default: 0}] \leavevmode
Sharing tags between devices.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
Tag set is not shared.
\\
\hline
1
&
Tag set shared between devices for blk\sphinxhyphen{}mq. Only makes sense with
nr\_devices \textgreater{} 1, otherwise there\textquotesingle{}s no tag set to share.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{zoned={[}0/1{]}: Default: 0}] \leavevmode
Device is a random\sphinxhyphen{}access or a zoned block device.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

0
&
Block device is exposed as a random\sphinxhyphen{}access block device.
\\
\hline
1
&
Block device is exposed as a host\sphinxhyphen{}managed zoned block device. Requires
CONFIG\_BLK\_DEV\_ZONED.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\item[{zone\_size={[}MB{]}: Default: 256}] \leavevmode
Per zone size when exposed as a zoned block device. Must be a power of two.

\item[{zone\_nr\_conv={[}nr\_conv{]}: Default: 0}] \leavevmode
The number of conventional zones to create when block device is zoned.  If
zone\_nr\_conv \textgreater{}= nr\_zones, it will be reduced to nr\_zones \sphinxhyphen{} 1.

\end{description}


\chapter{Block layer support for Persistent Reservations}
\label{\detokenize{pr:block-layer-support-for-persistent-reservations}}\label{\detokenize{pr::doc}}
The Linux kernel supports a user space interface for simplified
Persistent Reservations which map to block devices that support
these (like SCSI). Persistent Reservations allow restricting
access to block devices to specific initiators in a shared storage
setup.

This document gives a general overview of the support ioctl commands.
For a more detailed reference please refer to the SCSI Primary
Commands standard, specifically the section on Reservations and the
"PERSISTENT RESERVE IN" and "PERSISTENT RESERVE OUT" commands.

All implementations are expected to ensure the reservations survive
a power loss and cover all connections in a multi path environment.
These behaviors are optional in SPC but will be automatically applied
by Linux.


\section{The following types of reservations are supported:}
\label{\detokenize{pr:the-following-types-of-reservations-are-supported}}\begin{itemize}
\item {} \begin{description}
\item[{PR\_WRITE\_EXCLUSIVE}] \leavevmode
Only the initiator that owns the reservation can write to the
device.  Any initiator can read from the device.

\end{description}

\item {} \begin{description}
\item[{PR\_EXCLUSIVE\_ACCESS}] \leavevmode
Only the initiator that owns the reservation can access the
device.

\end{description}

\item {} \begin{description}
\item[{PR\_WRITE\_EXCLUSIVE\_REG\_ONLY}] \leavevmode
Only initiators with a registered key can write to the device,
Any initiator can read from the device.

\end{description}

\item {} \begin{description}
\item[{PR\_EXCLUSIVE\_ACCESS\_REG\_ONLY}] \leavevmode
Only initiators with a registered key can access the device.

\end{description}

\item {} 
PR\_WRITE\_EXCLUSIVE\_ALL\_REGS
\begin{quote}

Only initiators with a registered key can write to the device,
Any initiator can read from the device.
All initiators with a registered key are considered reservation
holders.
Please reference the SPC spec on the meaning of a reservation
holder if you want to use this type.
\end{quote}

\item {} \begin{description}
\item[{PR\_EXCLUSIVE\_ACCESS\_ALL\_REGS}] \leavevmode
Only initiators with a registered key can access the device.
All initiators with a registered key are considered reservation
holders.
Please reference the SPC spec on the meaning of a reservation
holder if you want to use this type.

\end{description}

\end{itemize}


\section{The following ioctl are supported:}
\label{\detokenize{pr:the-following-ioctl-are-supported}}

\subsection{1. IOC\_PR\_REGISTER}
\label{\detokenize{pr:ioc-pr-register}}
This ioctl command registers a new reservation if the new\_key argument
is non\sphinxhyphen{}null.  If no existing reservation exists old\_key must be zero,
if an existing reservation should be replaced old\_key must contain
the old reservation key.

If the new\_key argument is 0 it unregisters the existing reservation passed
in old\_key.


\subsection{2. IOC\_PR\_RESERVE}
\label{\detokenize{pr:ioc-pr-reserve}}
This ioctl command reserves the device and thus restricts access for other
devices based on the type argument.  The key argument must be the existing
reservation key for the device as acquired by the IOC\_PR\_REGISTER,
IOC\_PR\_REGISTER\_IGNORE, IOC\_PR\_PREEMPT or IOC\_PR\_PREEMPT\_ABORT commands.


\subsection{3. IOC\_PR\_RELEASE}
\label{\detokenize{pr:ioc-pr-release}}
This ioctl command releases the reservation specified by key and flags
and thus removes any access restriction implied by it.


\subsection{4. IOC\_PR\_PREEMPT}
\label{\detokenize{pr:ioc-pr-preempt}}
This ioctl command releases the existing reservation referred to by
old\_key and replaces it with a new reservation of type for the
reservation key new\_key.


\subsection{5. IOC\_PR\_PREEMPT\_ABORT}
\label{\detokenize{pr:ioc-pr-preempt-abort}}
This ioctl command works like IOC\_PR\_PREEMPT except that it also aborts
any outstanding command sent over a connection identified by old\_key.


\subsection{6. IOC\_PR\_CLEAR}
\label{\detokenize{pr:ioc-pr-clear}}
This ioctl command unregisters both key and any other reservation key
registered with the device and drops any existing reservation.


\section{Flags}
\label{\detokenize{pr:flags}}
All the ioctls have a flag field.  Currently only one flag is supported:
\begin{itemize}
\item {} \begin{description}
\item[{PR\_FL\_IGNORE\_KEY}] \leavevmode
Ignore the existing reservation key.  This is commonly supported for
IOC\_PR\_REGISTER, and some implementation may support the flag for
IOC\_PR\_RESERVE.

\end{description}

\end{itemize}

For all unknown flags the kernel will return \sphinxhyphen{}EOPNOTSUPP.


\chapter{Block layer statistics in /sys/block/\textless{}dev\textgreater{}/stat}
\label{\detokenize{stat:block-layer-statistics-in-sys-block-dev-stat}}\label{\detokenize{stat::doc}}
This file documents the contents of the /sys/block/\textless{}dev\textgreater{}/stat file.

The stat file provides several statistics about the state of block
device \textless{}dev\textgreater{}.
\begin{enumerate}
\sphinxsetlistlabels{\Alph}{enumi}{enumii}{}{.}%
\setcounter{enumi}{16}
\item {} 
Why are there multiple statistics in a single file?  Doesn\textquotesingle{}t sysfs
normally contain a single value per file?

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\Alph}{enumi}{enumii}{}{.}%
\item {} 
By having a single file, the kernel can guarantee that the statistics
represent a consistent snapshot of the state of the device.  If the
statistics were exported as multiple files containing one statistic
each, it would be impossible to guarantee that a set of readings
represent a single point in time.

\end{enumerate}

The stat file consists of a single line of text containing 17 decimal
values separated by whitespace.  The fields are summarized in the
following table, and described in more detail below.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
Name
&\sphinxstyletheadfamily 
units
&\sphinxstyletheadfamily 
description
\\
\hline
read I/Os
&
requests
&
number of read I/Os processed
\\
\hline
read merges
&
requests
&
number of read I/Os merged with in\sphinxhyphen{}queue I/O
\\
\hline
read sectors
&
sectors
&
number of sectors read
\\
\hline
read ticks
&
milliseconds
&
total wait time for read requests
\\
\hline
write I/Os
&
requests
&
number of write I/Os processed
\\
\hline
write merges
&
requests
&
number of write I/Os merged with in\sphinxhyphen{}queue I/O
\\
\hline
write sectors
&
sectors
&
number of sectors written
\\
\hline
write ticks
&
milliseconds
&
total wait time for write requests
\\
\hline
in\_flight
&
requests
&
number of I/Os currently in flight
\\
\hline
io\_ticks
&
milliseconds
&
total time this block device has been active
\\
\hline
time\_in\_queue
&
milliseconds
&
total wait time for all requests
\\
\hline
discard I/Os
&
requests
&
number of discard I/Os processed
\\
\hline
discard merges
&
requests
&
number of discard I/Os merged with in\sphinxhyphen{}queue I/O
\\
\hline
discard sectors
&
sectors
&
number of sectors discarded
\\
\hline
discard ticks
&
milliseconds
&
total wait time for discard requests
\\
\hline
flush I/Os
&
requests
&
number of flush I/Os processed
\\
\hline
flush ticks
&
milliseconds
&
total wait time for flush requests
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{read I/Os, write I/Os, discard I/0s}
\label{\detokenize{stat:read-i-os-write-i-os-discard-i-0s}}
These values increment when an I/O request completes.


\section{flush I/Os}
\label{\detokenize{stat:flush-i-os}}
These values increment when an flush I/O request completes.

Block layer combines flush requests and executes at most one at a time.
This counts flush requests executed by disk. Not tracked for partitions.


\section{read merges, write merges, discard merges}
\label{\detokenize{stat:read-merges-write-merges-discard-merges}}
These values increment when an I/O request is merged with an
already\sphinxhyphen{}queued I/O request.


\section{read sectors, write sectors, discard\_sectors}
\label{\detokenize{stat:read-sectors-write-sectors-discard-sectors}}
These values count the number of sectors read from, written to, or
discarded from this block device.  The "sectors" in question are the
standard UNIX 512\sphinxhyphen{}byte sectors, not any device\sphinxhyphen{} or filesystem\sphinxhyphen{}specific
block size.  The counters are incremented when the I/O completes.


\section{read ticks, write ticks, discard ticks, flush ticks}
\label{\detokenize{stat:read-ticks-write-ticks-discard-ticks-flush-ticks}}
These values count the number of milliseconds that I/O requests have
waited on this block device.  If there are multiple I/O requests waiting,
these values will increase at a rate greater than 1000/second; for
example, if 60 read requests wait for an average of 30 ms, the read\_ticks
field will increase by 60*30 = 1800.


\section{in\_flight}
\label{\detokenize{stat:in-flight}}
This value counts the number of I/O requests that have been issued to
the device driver but have not yet completed.  It does not include I/O
requests that are in the queue but not yet issued to the device driver.


\section{io\_ticks}
\label{\detokenize{stat:io-ticks}}
This value counts the number of milliseconds during which the device has
had I/O requests queued.


\section{time\_in\_queue}
\label{\detokenize{stat:time-in-queue}}
This value counts the number of milliseconds that I/O requests have waited
on this block device.  If there are multiple I/O requests waiting, this
value will increase as the product of the number of milliseconds times the
number of requests waiting (see "read ticks" above for an example).


\chapter{Switching Scheduler}
\label{\detokenize{switching-sched:switching-scheduler}}\label{\detokenize{switching-sched::doc}}
Each io queue has a set of io scheduler tunables associated with it. These
tunables control how the io scheduler works. You can find these entries
in:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/sys/block/\PYGZlt{}device\PYGZgt{}/queue/iosched
\end{sphinxVerbatim}

assuming that you have sysfs mounted on /sys. If you don\textquotesingle{}t have sysfs mounted,
you can do so by typing:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} mount none /sys \PYGZhy{}t sysfs
\end{sphinxVerbatim}

It is possible to change the IO scheduler for a given block device on
the fly to select one of mq\sphinxhyphen{}deadline, none, bfq, or kyber schedulers \sphinxhyphen{}
which can improve that device\textquotesingle{}s throughput.

To set a specific scheduler, simply do this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
echo SCHEDNAME \PYGZgt{} /sys/block/DEV/queue/scheduler
\end{sphinxVerbatim}

where SCHEDNAME is the name of a defined IO scheduler, and DEV is the
device name (hda, hdb, sga, or whatever you happen to have).

The list of defined schedulers can be found by simply doing
a "cat /sys/block/DEV/queue/scheduler" \sphinxhyphen{} the list of valid names
will be displayed, with the currently selected scheduler in brackets:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} cat /sys/block/sda/queue/scheduler
[mq\PYGZhy{}deadline] kyber bfq none
\PYGZsh{} echo none \PYGZgt{}/sys/block/sda/queue/scheduler
\PYGZsh{} cat /sys/block/sda/queue/scheduler
[none] mq\PYGZhy{}deadline kyber bfq
\end{sphinxVerbatim}


\chapter{Explicit volatile write back cache control}
\label{\detokenize{writeback_cache_control:explicit-volatile-write-back-cache-control}}\label{\detokenize{writeback_cache_control::doc}}

\section{Introduction}
\label{\detokenize{writeback_cache_control:introduction}}
Many storage devices, especially in the consumer market, come with volatile
write back caches.  That means the devices signal I/O completion to the
operating system before data actually has hit the non\sphinxhyphen{}volatile storage.  This
behavior obviously speeds up various workloads, but it means the operating
system needs to force data out to the non\sphinxhyphen{}volatile storage when it performs
a data integrity operation like fsync, sync or an unmount.

The Linux block layer provides two simple mechanisms that let filesystems
control the caching behavior of the storage device.  These mechanisms are
a forced cache flush, and the Force Unit Access (FUA) flag for requests.


\section{Explicit cache flushes}
\label{\detokenize{writeback_cache_control:explicit-cache-flushes}}
The REQ\_PREFLUSH flag can be OR ed into the r/w flags of a bio submitted from
the filesystem and will make sure the volatile cache of the storage device
has been flushed before the actual I/O operation is started.  This explicitly
guarantees that previously completed write requests are on non\sphinxhyphen{}volatile
storage before the flagged bio starts. In addition the REQ\_PREFLUSH flag can be
set on an otherwise empty bio structure, which causes only an explicit cache
flush without any dependent I/O.  It is recommend to use
the blkdev\_issue\_flush() helper for a pure cache flush.


\section{Forced Unit Access}
\label{\detokenize{writeback_cache_control:forced-unit-access}}
The REQ\_FUA flag can be OR ed into the r/w flags of a bio submitted from the
filesystem and will make sure that I/O completion for this request is only
signaled after the data has been committed to non\sphinxhyphen{}volatile storage.


\section{Implementation details for filesystems}
\label{\detokenize{writeback_cache_control:implementation-details-for-filesystems}}
Filesystems can simply set the REQ\_PREFLUSH and REQ\_FUA bits and do not have to
worry if the underlying devices need any explicit cache flushing and how
the Forced Unit Access is implemented.  The REQ\_PREFLUSH and REQ\_FUA flags
may both be set on a single bio.


\section{Implementation details for bio based block drivers}
\label{\detokenize{writeback_cache_control:implementation-details-for-bio-based-block-drivers}}
These drivers will always see the REQ\_PREFLUSH and REQ\_FUA bits as they sit
directly below the submit\_bio interface.  For remapping drivers the REQ\_FUA
bits need to be propagated to underlying devices, and a global flush needs
to be implemented for bios with the REQ\_PREFLUSH bit set.  For real device
drivers that do not have a volatile cache the REQ\_PREFLUSH and REQ\_FUA bits
on non\sphinxhyphen{}empty bios can simply be ignored, and REQ\_PREFLUSH requests without
data can be completed successfully without doing any work.  Drivers for
devices with volatile caches need to implement the support for these
flags themselves without any help from the block layer.


\section{Implementation details for request\_fn based block drivers}
\label{\detokenize{writeback_cache_control:implementation-details-for-request-fn-based-block-drivers}}
For devices that do not support volatile write caches there is no driver
support required, the block layer completes empty REQ\_PREFLUSH requests before
entering the driver and strips off the REQ\_PREFLUSH and REQ\_FUA bits from
requests that have a payload.  For devices with volatile write caches the
driver needs to tell the block layer that it supports flushing caches by
doing:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
blk\PYGZus{}queue\PYGZus{}write\PYGZus{}cache(sdkp\PYGZhy{}\PYGZgt{}disk\PYGZhy{}\PYGZgt{}queue, true, false);
\end{sphinxVerbatim}

and handle empty REQ\_OP\_FLUSH requests in its prep\_fn/request\_fn.  Note that
REQ\_PREFLUSH requests with a payload are automatically turned into a sequence
of an empty REQ\_OP\_FLUSH request followed by the actual write by the block
layer.  For devices that also support the FUA bit the block layer needs
to be told to pass through the REQ\_FUA bit using:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
blk\PYGZus{}queue\PYGZus{}write\PYGZus{}cache(sdkp\PYGZhy{}\PYGZgt{}disk\PYGZhy{}\PYGZgt{}queue, true, true);
\end{sphinxVerbatim}

and the driver must handle write requests that have the REQ\_FUA bit set
in prep\_fn/request\_fn.  If the FUA bit is not natively supported the block
layer turns it into an empty REQ\_OP\_FLUSH request after the actual write.


\chapter{Userspace block device driver (ublk driver)}
\label{\detokenize{ublk:userspace-block-device-driver-ublk-driver}}\label{\detokenize{ublk::doc}}

\section{Overview}
\label{\detokenize{ublk:overview}}
ublk is a generic framework for implementing block device logic from userspace.
The motivation behind it is that moving virtual block drivers into userspace,
such as loop, nbd and similar can be very helpful. It can help to implement
new virtual block device such as ublk\sphinxhyphen{}qcow2 (there are several attempts of
implementing qcow2 driver in kernel).

Userspace block devices are attractive because:
\begin{itemize}
\item {} 
They can be written many programming languages.

\item {} 
They can use libraries that are not available in the kernel.

\item {} 
They can be debugged with tools familiar to application developers.

\item {} 
Crashes do not kernel panic the machine.

\item {} 
Bugs are likely to have a lower security impact than bugs in kernel
code.

\item {} 
They can be installed and updated independently of the kernel.

\item {} 
They can be used to simulate block device easily with user specified
parameters/setting for test/debug purpose

\end{itemize}

ublk block device (\sphinxcode{\sphinxupquote{/dev/ublkb*}}) is added by ublk driver. Any IO request
on the device will be forwarded to ublk userspace program. For convenience,
in this document, \sphinxcode{\sphinxupquote{ublk server}} refers to generic ublk userspace
program. \sphinxcode{\sphinxupquote{ublksrv}} %
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxurl{https://github.com/ming1/ubdsrv}
%
\end{footnote} is one of such implementation. It
provides \sphinxcode{\sphinxupquote{libublksrv}} %
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxurl{https://github.com/ming1/ubdsrv/tree/master/lib}
%
\end{footnote} library for developing specific
user block device conveniently, while also generic type block device is
included, such as loop and null. Richard W.M. Jones wrote userspace nbd device
\sphinxcode{\sphinxupquote{nbdublk}} %
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxurl{https://gitlab.com/rwmjones/libnbd/-/tree/nbdublk}
%
\end{footnote}  based on \sphinxcode{\sphinxupquote{libublksrv}} \sphinxfootnotemark[2].

After the IO is handled by userspace, the result is committed back to the
driver, thus completing the request cycle. This way, any specific IO handling
logic is totally done by userspace, such as loop\textquotesingle{}s IO handling, NBD\textquotesingle{}s IO
communication, or qcow2\textquotesingle{}s IO mapping.

\sphinxcode{\sphinxupquote{/dev/ublkb*}} is driven by blk\sphinxhyphen{}mq request\sphinxhyphen{}based driver. Each request is
assigned by one queue wide unique tag. ublk server assigns unique tag to each
IO too, which is 1:1 mapped with IO of \sphinxcode{\sphinxupquote{/dev/ublkb*}}.

Both the IO request forward and IO handling result committing are done via
\sphinxcode{\sphinxupquote{io\_uring}} passthrough command; that is why ublk is also one io\_uring based
block driver. It has been observed that using io\_uring passthrough command can
give better IOPS than block IO; which is why ublk is one of high performance
implementation of userspace block device: not only IO request communication is
done by io\_uring, but also the preferred IO handling in ublk server is io\_uring
based approach too.

ublk provides control interface to set/get ublk block device parameters.
The interface is extendable and kabi compatible: basically any ublk request
queue\textquotesingle{}s parameter or ublk generic feature parameters can be set/get via the
interface. Thus, ublk is generic userspace block device framework.
For example, it is easy to setup a ublk device with specified block
parameters from userspace.


\section{Using ublk}
\label{\detokenize{ublk:using-ublk}}
ublk requires userspace ublk server to handle real block device logic.

Below is example of using \sphinxcode{\sphinxupquote{ublksrv}} to provide ublk\sphinxhyphen{}based loop device.
\begin{itemize}
\item {} 
add a device:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ublk add \PYGZhy{}t loop \PYGZhy{}f ublk\PYGZhy{}loop.img
\end{sphinxVerbatim}

\item {} 
format with xfs, then use it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkfs.xfs /dev/ublkb0
mount /dev/ublkb0 /mnt
\PYGZsh{} do anything. all IOs are handled by io\PYGZus{}uring
...
umount /mnt
\end{sphinxVerbatim}

\item {} 
list the devices with their info:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ublk list
\end{sphinxVerbatim}

\item {} 
delete the device:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ublk del \PYGZhy{}a
ublk del \PYGZhy{}n \PYGZdl{}ublk\PYGZus{}dev\PYGZus{}id
\end{sphinxVerbatim}

\end{itemize}

See usage details in README of \sphinxcode{\sphinxupquote{ublksrv}} %
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxurl{https://github.com/ming1/ubdsrv/blob/master/README}
%
\end{footnote}.


\section{Design}
\label{\detokenize{ublk:design}}

\subsection{Control plane}
\label{\detokenize{ublk:control-plane}}
ublk driver provides global misc device node (\sphinxcode{\sphinxupquote{/dev/ublk\sphinxhyphen{}control}}) for
managing and controlling ublk devices with help of several control commands:
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_ADD\_DEV}}

Add a ublk char device (\sphinxcode{\sphinxupquote{/dev/ublkc*}}) which is talked with ublk server
WRT IO command communication. Basic device info is sent together with this
command. It sets UAPI structure of \sphinxcode{\sphinxupquote{ublksrv\_ctrl\_dev\_info}},
such as \sphinxcode{\sphinxupquote{nr\_hw\_queues}}, \sphinxcode{\sphinxupquote{queue\_depth}}, and max IO request buffer size,
for which the info is negotiated with the driver and sent back to the server.
When this command is completed, the basic device info is immutable.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_SET\_PARAMS}} / \sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_PARAMS}}

Set or get parameters of the device, which can be either generic feature
related, or request queue limit related, but can\textquotesingle{}t be IO logic specific,
because the driver does not handle any IO logic. This command has to be
sent before sending \sphinxcode{\sphinxupquote{UBLK\_CMD\_START\_DEV}}.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_START\_DEV}}

After the server prepares userspace resources (such as creating per\sphinxhyphen{}queue
pthread \& io\_uring for handling ublk IO), this command is sent to the
driver for allocating \& exposing \sphinxcode{\sphinxupquote{/dev/ublkb*}}. Parameters set via
\sphinxcode{\sphinxupquote{UBLK\_CMD\_SET\_PARAMS}} are applied for creating the device.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_STOP\_DEV}}

Halt IO on \sphinxcode{\sphinxupquote{/dev/ublkb*}} and remove the device. When this command returns,
ublk server will release resources (such as destroying per\sphinxhyphen{}queue pthread \&
io\_uring).

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_DEL\_DEV}}

Remove \sphinxcode{\sphinxupquote{/dev/ublkc*}}. When this command returns, the allocated ublk device
number can be reused.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_QUEUE\_AFFINITY}}

When \sphinxcode{\sphinxupquote{/dev/ublkc}} is added, the driver creates block layer tagset, so
that each queue\textquotesingle{}s affinity info is available. The server sends
\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_QUEUE\_AFFINITY}} to retrieve queue affinity info. It can
set up the per\sphinxhyphen{}queue context efficiently, such as bind affine CPUs with IO
pthread and try to allocate buffers in IO thread context.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO}}

For retrieving device info via \sphinxcode{\sphinxupquote{ublksrv\_ctrl\_dev\_info}}. It is the server\textquotesingle{}s
responsibility to save IO target specific info in userspace.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO2}}
Same purpose with \sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO}}, but ublk server has to
provide path of the char device of \sphinxcode{\sphinxupquote{/dev/ublkc*}} for kernel to run
permission check, and this command is added for supporting unprivileged
ublk device, and introduced with \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}} together.
Only the user owning the requested device can retrieve the device info.

How to deal with userspace/kernel compatibility:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
if kernel is capable of handling \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}

\end{enumerate}
\begin{quote}

If ublk server supports \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}:

ublk server should send \sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO2}}, given anytime
unprivileged application needs to query devices the current user owns,
when the application has no idea if \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}} is set
given the capability info is stateless, and application should always
retrieve it via \sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO2}}

If ublk server doesn\textquotesingle{}t support \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}:

\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO}} is always sent to kernel, and the feature of
UBLK\_F\_UNPRIVILEGED\_DEV isn\textquotesingle{}t available for user
\end{quote}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\setcounter{enumi}{1}
\item {} 
if kernel isn\textquotesingle{}t capable of handling \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}

\end{enumerate}
\begin{quote}

If ublk server supports \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}:

\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO2}} is tried first, and will be failed, then
\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO}} needs to be retried given
\sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}} can\textquotesingle{}t be set

If ublk server doesn\textquotesingle{}t support \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}:

\sphinxcode{\sphinxupquote{UBLK\_CMD\_GET\_DEV\_INFO}} is always sent to kernel, and the feature of
\sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}} isn\textquotesingle{}t available for user
\end{quote}

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_START\_USER\_RECOVERY}}

This command is valid if \sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY}} feature is enabled. This
command is accepted after the old process has exited, ublk device is quiesced
and \sphinxcode{\sphinxupquote{/dev/ublkc*}} is released. User should send this command before he starts
a new process which re\sphinxhyphen{}opens \sphinxcode{\sphinxupquote{/dev/ublkc*}}. When this command returns, the
ublk device is ready for the new process.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_CMD\_END\_USER\_RECOVERY}}

This command is valid if \sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY}} feature is enabled. This
command is accepted after ublk device is quiesced and a new process has
opened \sphinxcode{\sphinxupquote{/dev/ublkc*}} and get all ublk queues be ready. When this command
returns, ublk device is unquiesced and new I/O requests are passed to the
new process.

\item {} 
user recovery feature description

Two new features are added for user recovery: \sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY}} and
\sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY\_REISSUE}}.

With \sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY}} set, after one ubq\_daemon(ublk server\textquotesingle{}s io
handler) is dying, ublk does not delete \sphinxcode{\sphinxupquote{/dev/ublkb*}} during the whole
recovery stage and ublk device ID is kept. It is ublk server\textquotesingle{}s
responsibility to recover the device context by its own knowledge.
Requests which have not been issued to userspace are requeued. Requests
which have been issued to userspace are aborted.

With \sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY\_REISSUE}} set, after one ubq\_daemon(ublk
server\textquotesingle{}s io handler) is dying, contrary to \sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY}},
requests which have been issued to userspace are requeued and will be
re\sphinxhyphen{}issued to the new process after handling \sphinxcode{\sphinxupquote{UBLK\_CMD\_END\_USER\_RECOVERY}}.
\sphinxcode{\sphinxupquote{UBLK\_F\_USER\_RECOVERY\_REISSUE}} is designed for backends who tolerate
double\sphinxhyphen{}write since the driver may issue the same I/O request twice. It
might be useful to a read\sphinxhyphen{}only FS or a VM backend.

\end{itemize}

Unprivileged ublk device is supported by passing \sphinxcode{\sphinxupquote{UBLK\_F\_UNPRIVILEGED\_DEV}}.
Once the flag is set, all control commands can be sent by unprivileged
user. Except for command of \sphinxcode{\sphinxupquote{UBLK\_CMD\_ADD\_DEV}}, permission check on
the specified char device(\sphinxcode{\sphinxupquote{/dev/ublkc*}}) is done for all other control
commands by ublk driver, for doing that, path of the char device has to
be provided in these commands\textquotesingle{} payload from ublk server. With this way,
ublk device becomes container\sphinxhyphen{}ware, and device created in one container
can be controlled/accessed just inside this container.


\subsection{Data plane}
\label{\detokenize{ublk:data-plane}}
ublk server needs to create per\sphinxhyphen{}queue IO pthread \& io\_uring for handling IO
commands via io\_uring passthrough. The per\sphinxhyphen{}queue IO pthread
focuses on IO handling and shouldn\textquotesingle{}t handle any control \& management
tasks.

The\textquotesingle{}s IO is assigned by a unique tag, which is 1:1 mapping with IO
request of \sphinxcode{\sphinxupquote{/dev/ublkb*}}.

UAPI structure of \sphinxcode{\sphinxupquote{ublksrv\_io\_desc}} is defined for describing each IO from
the driver. A fixed mmaped area (array) on \sphinxcode{\sphinxupquote{/dev/ublkc*}} is provided for
exporting IO info to the server; such as IO offset, length, OP/flags and
buffer address. Each \sphinxcode{\sphinxupquote{ublksrv\_io\_desc}} instance can be indexed via queue id
and IO tag directly.

The following IO commands are communicated via io\_uring passthrough command,
and each command is only for forwarding the IO and committing the result
with specified IO tag in the command data:
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{UBLK\_IO\_FETCH\_REQ}}

Sent from the server IO pthread for fetching future incoming IO requests
destined to \sphinxcode{\sphinxupquote{/dev/ublkb*}}. This command is sent only once from the server
IO pthread for ublk driver to setup IO forward environment.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_IO\_COMMIT\_AND\_FETCH\_REQ}}

When an IO request is destined to \sphinxcode{\sphinxupquote{/dev/ublkb*}}, the driver stores
the IO\textquotesingle{}s \sphinxcode{\sphinxupquote{ublksrv\_io\_desc}} to the specified mapped area; then the
previous received IO command of this IO tag (either \sphinxcode{\sphinxupquote{UBLK\_IO\_FETCH\_REQ}}
or \sphinxcode{\sphinxupquote{UBLK\_IO\_COMMIT\_AND\_FETCH\_REQ)}} is completed, so the server gets
the IO notification via io\_uring.

After the server handles the IO, its result is committed back to the
driver by sending \sphinxcode{\sphinxupquote{UBLK\_IO\_COMMIT\_AND\_FETCH\_REQ}} back. Once ublkdrv
received this command, it parses the result and complete the request to
\sphinxcode{\sphinxupquote{/dev/ublkb*}}. In the meantime setup environment for fetching future
requests with the same IO tag. That is, \sphinxcode{\sphinxupquote{UBLK\_IO\_COMMIT\_AND\_FETCH\_REQ}}
is reused for both fetching request and committing back IO result.

\item {} 
\sphinxcode{\sphinxupquote{UBLK\_IO\_NEED\_GET\_DATA}}

With \sphinxcode{\sphinxupquote{UBLK\_F\_NEED\_GET\_DATA}} enabled, the WRITE request will be firstly
issued to ublk server without data copy. Then, IO backend of ublk server
receives the request and it can allocate data buffer and embed its addr
inside this new io command. After the kernel driver gets the command,
data copy is done from request pages to this backend\textquotesingle{}s buffer. Finally,
backend receives the request again with data to be written and it can
truly handle the request.

\sphinxcode{\sphinxupquote{UBLK\_IO\_NEED\_GET\_DATA}} adds one additional round\sphinxhyphen{}trip and one
io\_uring\_enter() syscall. Any user thinks that it may lower performance
should not enable UBLK\_F\_NEED\_GET\_DATA. ublk server pre\sphinxhyphen{}allocates IO
buffer for each IO by default. Any new project should try to use this
buffer to communicate with ublk driver. However, existing project may
break or not able to consume the new buffer interface; that\textquotesingle{}s why this
command is added for backwards compatibility so that existing projects
can still consume existing buffers.

\item {} 
data copy between ublk server IO buffer and ublk block IO request

The driver needs to copy the block IO request pages into the server buffer
(pages) first for WRITE before notifying the server of the coming IO, so
that the server can handle WRITE request.

When the server handles READ request and sends
\sphinxcode{\sphinxupquote{UBLK\_IO\_COMMIT\_AND\_FETCH\_REQ}} to the server, ublkdrv needs to copy
the server buffer (pages) read to the IO request pages.

\end{itemize}


\section{Future development}
\label{\detokenize{ublk:future-development}}

\subsection{Zero copy}
\label{\detokenize{ublk:zero-copy}}
Zero copy is a generic requirement for nbd, fuse or similar drivers. A
problem %
\begin{footnote}[6]\sphinxAtStartFootnote
\sphinxurl{https://lore.kernel.org/linux-block/YoOr6jBfgVm8GvWg@stefanha-x1.localdomain/}
%
\end{footnote} Xiaoguang mentioned is that pages mapped to userspace
can\textquotesingle{}t be remapped any more in kernel with existing mm interfaces. This can
occurs when destining direct IO to \sphinxcode{\sphinxupquote{/dev/ublkb*}}. Also, he reported that
big requests (IO size \textgreater{}= 256 KB) may benefit a lot from zero copy.


\section{References}
\label{\detokenize{ublk:references}}


\renewcommand{\indexname}{Index}
\printindex
\end{document}